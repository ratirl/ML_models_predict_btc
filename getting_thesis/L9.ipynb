{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part one: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas_ta as ta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -.- is to remove the scientific notation of the log_returns in numpy arrays\n",
    "pd.set_option('display.float_format', str)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nflx_intraday = pd.read_csv('data_nflx_intraday.csv', index_col=0, parse_dates=True) # will be our working df\n",
    "df = df_nflx_intraday.copy() # working df\n",
    "df.columns = df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding buy signals and their respective technical indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding some indicators\n",
    "#super trend : calculates the trend, length is the ATR length default is 7 but i took 10 since i wanna trade on the 10 minute horizon\n",
    "# the signal is 1 if the trend is going up aka buy signal\n",
    "# the value of supertrend is a price which ill use later as a feature\n",
    "supertrend = df.ta.supertrend(length = 10)\n",
    "supertrend.rename(columns={'SUPERTd_10_3.0': 'supertrend_signal', 'SUPERT_10_3.0' : 'supertrend'}, inplace=True)\n",
    "supertrend = supertrend[['supertrend_signal','supertrend']] # <- first df to append to main df\n",
    "df = pd.concat([df, supertrend], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VWAP\n",
    "df.ta.vwap(append=True)\n",
    "df.rename(columns={'VWAP_D': 'vwap'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vwap_signal'] = (df['vwap'] < df['close']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stoch indicator and signal\n",
    "stoch = df.ta.stoch(k=10, d=2)\n",
    "stoch.rename(columns={'STOCHk_10_2_3': 'stoch_k', 'STOCHd_10_2_3' : 'stoch_d'}, inplace=True)\n",
    "stoch['stoch_signal'] = ((stoch['stoch_k'] <= 20) & (stoch['stoch_k'] > stoch['stoch_d'])).astype(int)\n",
    "df = pd.concat([df, stoch], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adx\n",
    "adx = df.ta.adx(length=10)\n",
    "adx.rename(columns={'ADX_10': 'adx', 'DMP_10' : 'adx_direction_pos', 'DMN_10' : 'adx_direction_neg'}, inplace=True)\n",
    "adx['adx_signal'] = ((adx['adx'] > 25) & (adx['adx_direction_pos'] > adx['adx_direction_neg'])).astype(int)\n",
    "df = pd.concat([df, adx], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obv\n",
    "# with the obv we want to check for divergence ( obv is increasing and price is decreasing, this could be a potential buy signal)\n",
    "obv = df.ta.obv()\n",
    "obv = pd.DataFrame(obv)\n",
    "obv.rename(columns={'OBV': 'obv'}, inplace=True)\n",
    "df = pd.concat([df, obv], axis=1)\n",
    "df['obv_signal'] = ((df['obv'] > df['obv'].rolling(window=5).mean()) & (df['close'] < df['close'].rolling(window=5).mean())).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rsi\n",
    "df['rsi'] = df.ta.rsi()\n",
    "df['rsi_signal'] = (df['rsi'] < 30).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# golden cross\n",
    "df['gc_signal'] = (ta.sma(df['close'], length=5) > ta.sma(df['close'], length=15)).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# donchian\n",
    "donchian = df.ta.donchian(lower_length=15, upper_length=15)\n",
    "donchian.rename(columns={'DCU_15_15': 'donchian_upper'}, inplace=True)\n",
    "df['donchian_signal'] = (donchian['donchian_upper'] > df['close']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# macd\n",
    "macd = df.ta.macd(fast=5, slow=15, signal=3)\n",
    "\n",
    "macd['macd_signal'] = (\n",
    "    (macd['MACD_5_15_3'] < 0) & (macd['MACDs_5_15_3'] < 0) &\n",
    "    (macd['MACD_5_15_3'] > macd['MACDs_5_15_3']) &\n",
    "    (macd['MACD_5_15_3'].shift(1) <= macd['MACDs_5_15_3'].shift(1)) &\n",
    "    (macd['MACDh_5_15_3'] > 0)).astype(int)\n",
    "\n",
    "macd.rename(columns={'MACDh_5_15_3': 'macd'}, inplace=True)\n",
    "df = pd.concat([df, macd[['macd', 'macd_signal']]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adx.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ta.macd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding some signals using visual patterns with candlestick trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.investopedia.com/articles/active-trading/062315/using-bullish-candlestick-patterns-buy-stocks.asp\n",
    "# candles df\n",
    "candles = df.ta.cdl_pattern(name=['engulfing', 'hammer', 'invertedhammer', 'piercing', 'morningstar', '3whitesoldiers'])\n",
    "candles.rename(columns={\n",
    "    'CDL_ENGULFING': 'engulfing',\n",
    "    'CDL_HAMMER': 'hammer',\n",
    "    'CDL_INVERTEDHAMMER': 'invertedhammer',\n",
    "    'CDL_PIERCING': 'piercing',\n",
    "    'CDL_MORNINGSTAR': 'morningstar',\n",
    "    'CDL_3WHITESOLDIERS': '3whitesoldiers'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, candles], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[1:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Did't need a lot of data processing as we didn't have tons of NaN data, now we will choose if we want regression or classiication\n",
    "Classification: using the features to predict y: (Bool price increase or no) -> logistic regression and random forest classifier\n",
    "\n",
    "Regression: using the features to predict y: (Continuous value price change) -> linear regression and random forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price_increased'] = np.where(df['close'].diff() > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification model one: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copying the df to main_df so we can copy back from it for future models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df_main = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing X and y\n",
    "X = df.drop(columns=['price_increased'])\n",
    "y = df[['price_increased']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_logistic = logistic_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_logistic)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy is 0.5483133218982276 which is not that good, the main issue i can think of is that our data is shuffled with the train_test_split function and our data is sequencial as it has DateTimeIndex as its index. One solution could be to not shuffle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_logistic = logistic_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_logistic)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our accuracy is 0.6921097770154374 but still not that great! What we could do next is standardize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "predictions = logistic_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now our accuracy is even better, 0.7764436821040595! Lets take 3 random values and see what the model predicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_try =  df[100:103].drop(columns=['price_increased'])\n",
    "X_try = scaler.fit_transform(X_try)\n",
    "y_try = df[100:103][['price_increased']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model.predict(X_try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_try"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification model two: Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_main.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "scaler = StandardScaler()  \n",
    "# dividing X and y\n",
    "X = df.drop(columns=['price_increased'])\n",
    "y = df[['price_increased']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "random_forest_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "random_forest_classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = random_forest_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test,predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "\n",
    "print(\"Random Forest Classifier Accuracy:\", accuracy)\n",
    "print(\"Random Forest Classifier Precision:\", precision)\n",
    "print(\"Random Forest Classifier Recall:\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A benefit of using random forests is that we can use the Feature importances function to get an an idea of which features are more important in predicting y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = random_forest_classifier.feature_importances_\n",
    "columns = df.drop(columns=['price_increased']).columns\n",
    "feature_importance_df = pd.DataFrame({'Feature': columns, 'Importance': feature_importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the macd, rsi, stoch, adx, volume, obv and vwap are the most important features in predicting wether the price will increase or decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will look into two regression models. Here we try to predict a continuous value ( best case scenario the future price).\n",
    "Instead of the price we will be calculating the change in price because its more static.\n",
    "First model will be a classic linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression model one: linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main differences are that we try to predict another y with our features and that the scoring is a little bit different. Instead of the accuracy we will try to have a low as possible RMSE (this is the error rate of our models predictions vs the actual values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "scaler = StandardScaler()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_main.copy()\n",
    "df['log_return'] = ta.log_return(df['close'])\n",
    "df.dropna(inplace=True)\n",
    "# dividing X and y\n",
    "X = df.drop(columns=['log_return'])\n",
    "y = df[['log_return']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "predictions = linear_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df.corr()\n",
    "correlation_matrix[\"log_return\"].sort_values(ascending=False)\n",
    "\n",
    "# here we can see the correlation between the close price and the other features\n",
    "# this could help us in deciding which columns to keep\n",
    "# value between -1 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our MSE is 3.374455314810467e-07 and once again we can see which features are the most correlated to our y prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression model two: Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "scaler = StandardScaler()  \n",
    "df = df_main.copy()\n",
    "df['log_return'] = ta.log_return(df['close'])\n",
    "df.dropna(inplace=True)\n",
    "# dividing X and y\n",
    "X = df.drop(columns=['log_return'])\n",
    "y = df[['log_return']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "\n",
    "predictions = random_forest_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"Random Forest Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = random_forest_model.feature_importances_\n",
    "columns = df.drop(columns=['log_return']).columns\n",
    "feature_importance_df = pd.DataFrame({'Feature': columns, 'Importance': feature_importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part two: Neural Networks: regression and classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_main.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price_increased'] = np.where(df['close'].diff() > 0, 1, 0)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing X and y\n",
    "X = df.drop(columns=['price_increased'])\n",
    "y = df[['price_increased']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** PAGE 296 **\n",
    "model = keras.models.Sequential([\n",
    " keras.layers.Flatten(input_shape=(num_features,)),\n",
    " keras.layers.Dense(300, activation=\"relu\"),\n",
    " keras.layers.Dense(100, activation=\"relu\"),\n",
    " keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=\"sgd\",\n",
    "                metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                     validation_split=0.1, batch_size=32 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(16, 9))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: this is really good since both the losses are falling and both the accuracies are increasing. And the fact that the curves are really close to each other this means we dont have overfitting happening. \n",
    "If we are not happy with the result we can tune the \n",
    "    hyperparameters ( number of layers, number of neurons per layer, type of activation for each hidden layer, the epochs, batch siz (it can be set in the fit() method using the\n",
    "batch_size argument, which defaults to 32)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the model to make predictions\n",
    "X_new = X_test[200:210]\n",
    "predictions = model.predict(X_new)\n",
    "predictions.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[200:210]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequantial style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting the close price using neural networks with 31 features\n",
    "\n",
    "# copying the df so we have a fresh df to work with\n",
    "df = df_main.copy()\n",
    "df.dropna(inplace=True)\n",
    "num_features = df.shape[1]\n",
    "# dividing X and y\n",
    "X = df.drop(columns=['close'])\n",
    "y = df[['close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of columns:\", num_features) # 31 features\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "num_features = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#** PAGE 296 **\n",
    "model = keras.models.Sequential([\n",
    " keras.layers.Dense(30, activation=\"relu\", input_shape=(num_features,)),\n",
    " keras.layers.Dense(1, activation=\"linear\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, validation_split=0.1, batch_size=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean squared error rate on regressive model predicting close price is:', model.evaluate(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making predictions\n",
    "X_new = X_test[200:210]\n",
    "predictions = model.predict(X_new)\n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[200:210]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional style (more adjustable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X_train.shape[1]\n",
    "input = keras.layers.Input(shape=num_features)\n",
    "hidden = keras.layers.Dense(30, activation=\"relu\")(input)\n",
    "concat = keras.layers.Concatenate()([input, hidden])\n",
    "output = keras.layers.Dense(1, activation=\"linear\")(concat)\n",
    "model = keras.models.Model(inputs=[input], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "           validation_split=0.1, batch_size=32 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making predictions\n",
    "X_new = X_test[100:110]\n",
    "predictions = model.predict(X_new)\n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[100:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions are really flawed and I can clearly see that the loss starts off small but then suddenly increases, the model doesn't work as supposed. In a bit I will handle parameter tuning and optimization in the hopes that that will fix my issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM regression (using sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "df = df_main.copy()\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(df)\n",
    "df_scaled = scaler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_index_to_predict = df.columns.get_loc('close')\n",
    "features_count = df.shape[1]\n",
    "val_future = 1 # pred next i days\n",
    "sequence_length = 3 # use prev j days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "close_arr = []\n",
    "\n",
    "for i in range(len(df_scaled) - sequence_length):\n",
    "    sequences.append(df_scaled[i:i + sequence_length, :])\n",
    "    close_arr.append(df_scaled[i + sequence_length, col_index_to_predict])\n",
    "\n",
    "sequences = np.array(sequences)\n",
    "close_arr = np.array(close_arr)\n",
    "\n",
    "print(sequences.shape)\n",
    "print(close_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sequences, close_arr, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, activation='relu', input_shape=(sequence_length, features_count), return_sequences=True))\n",
    "# in the code above return_sequence true because we want the first ltsm to return another sequence\n",
    "# for the ltsm thats about to come, if the next layer isnt an lstm then set return to false\n",
    "model.add(LSTM(64, activation='relu', return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='linear'))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, batch_size=20, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean squared error rate on the sequential LSTM predicting close price is:', model.evaluate(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_pred = model.predict(X_test[12:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.tile(random_pred, (1, features_count))\n",
    "random_pred_scaled_back = scaler.inverse_transform(preds)[:, col_index_to_predict]\n",
    "\n",
    "real_y = y_test[12:13]\n",
    "reals = np.tile(real_y, (1, features_count))\n",
    "real_y_scaled_back = scaler.inverse_transform(reals)[:, col_index_to_predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predicted next price: ' ,random_pred_scaled_back, 'Actual next price: ', real_y_scaled_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the weights of the input layer\n",
    "input_weights = model.layers[0].get_weights()[0]\n",
    "\n",
    "# dictionary to store each weight to its column\n",
    "weights_by_column = dict(zip(X.columns.tolist(), input_weights.T))\n",
    "\n",
    "feature_weights = {feature: np.abs(weight).sum() for feature, weight in weights_by_column.items()}\n",
    "top_10_features = sorted(feature_weights, key=lambda x: feature_weights[x], reverse=True)[:10]\n",
    "for feature in top_10_features:\n",
    "    print(f\"Feature: {feature}, Weight: {feature_weights[feature]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM classification (using sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1669,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "df = df_main.copy()\n",
    "X = data \n",
    "y = data['price_increased']  \n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1670,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_index_to_predict = df.columns.get_loc('price_increased')\n",
    "num_features = df.shape[1]\n",
    "val_future = 1 # pred next i days\n",
    "sequence_length = 5 # use prev j days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1671,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17482, 5, 31)\n",
      "(17482,)\n"
     ]
    }
   ],
   "source": [
    "sequences = []\n",
    "price_increased_arr = []\n",
    "\n",
    "for i in range(len(X_scaled) - sequence_length):\n",
    "    sequences.append(X_scaled[i:i + sequence_length])\n",
    "    price_increased_arr.append(y.iloc[i + sequence_length - 1])\n",
    "\n",
    "sequences = np.array(sequences)\n",
    "price_increased_arr = np.array(price_increased_arr)\n",
    "\n",
    "sequences = np.array(sequences)\n",
    "price_increased_arr = np.array(price_increased_arr)\n",
    "\n",
    "print(sequences.shape)\n",
    "print(price_increased_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1672,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sequences, price_increased_arr, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1673,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_476\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_64 (LSTM)              (None, 5, 64)             24576     \n",
      "                                                                 \n",
      " lstm_65 (LSTM)              (None, 32)                12416     \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1991 (Dense)          (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 37,025\n",
      "Trainable params: 37,025\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, activation='tanh', input_shape=(sequence_length, num_features), return_sequences=True))\n",
    "model.add(LSTM(32, activation='tanh', return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics='accuracy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1674,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "630/630 [==============================] - 5s 4ms/step - loss: 0.6832 - accuracy: 0.5555 - val_loss: 0.6831 - val_accuracy: 0.5447\n",
      "Epoch 2/20\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.6744 - accuracy: 0.5776 - val_loss: 0.6774 - val_accuracy: 0.5590\n",
      "Epoch 3/20\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.6661 - accuracy: 0.6042 - val_loss: 0.6702 - val_accuracy: 0.5804\n",
      "Epoch 4/20\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.6539 - accuracy: 0.6270 - val_loss: 0.6617 - val_accuracy: 0.6097\n",
      "Epoch 5/20\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.6397 - accuracy: 0.6486 - val_loss: 0.6523 - val_accuracy: 0.6183\n",
      "Epoch 6/20\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.6213 - accuracy: 0.6669 - val_loss: 0.6386 - val_accuracy: 0.6483\n",
      "Epoch 7/20\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.5907 - accuracy: 0.7028 - val_loss: 0.6221 - val_accuracy: 0.6605\n",
      "Epoch 8/20\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.5420 - accuracy: 0.7427 - val_loss: 0.5999 - val_accuracy: 0.6998\n",
      "Epoch 9/20\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.4582 - accuracy: 0.8126 - val_loss: 0.5713 - val_accuracy: 0.7355\n",
      "Epoch 10/20\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.3292 - accuracy: 0.9225 - val_loss: 0.5389 - val_accuracy: 0.7484\n",
      "Epoch 11/20\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.1969 - accuracy: 0.9775 - val_loss: 0.4936 - val_accuracy: 0.7770\n",
      "Epoch 12/20\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.1147 - accuracy: 0.9935 - val_loss: 0.4329 - val_accuracy: 0.8320\n",
      "Epoch 13/20\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0721 - accuracy: 0.9985 - val_loss: 0.3640 - val_accuracy: 0.8935\n",
      "Epoch 14/20\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0479 - accuracy: 0.9996 - val_loss: 0.2984 - val_accuracy: 0.9400\n",
      "Epoch 15/20\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0346 - accuracy: 0.9998 - val_loss: 0.2421 - val_accuracy: 0.9693\n",
      "Epoch 16/20\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0253 - accuracy: 0.9999 - val_loss: 0.1978 - val_accuracy: 0.9886\n",
      "Epoch 17/20\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0195 - accuracy: 1.0000 - val_loss: 0.1629 - val_accuracy: 0.9957\n",
      "Epoch 18/20\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0158 - accuracy: 0.9999 - val_loss: 0.1363 - val_accuracy: 0.9993\n",
      "Epoch 19/20\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.1157 - val_accuracy: 0.9993\n",
      "Epoch 20/20\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.0995 - val_accuracy: 0.9993\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20, batch_size=20, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1666,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 0s 1ms/step - loss: 0.0810 - accuracy: 0.9997\n",
      "Accuracy on the sequential LSTM predicting if price will increase (y/n):  0.9997140169143677\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.0810 - accuracy: 0.9997\n",
      "Loss on the test set of the model:  0.08104927092790604\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy on the sequential LSTM predicting if price will increase (y/n): ', (model.evaluate(X_test,y_test))[1])\n",
    "print('Loss on the test set of the model: ', (model.evaluate(X_test,y_test))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1698,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n",
      "Predicted Labels: [[1]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>supertrend_signal</th>\n",
       "      <th>supertrend</th>\n",
       "      <th>vwap</th>\n",
       "      <th>vwap_signal</th>\n",
       "      <th>stoch_k</th>\n",
       "      <th>...</th>\n",
       "      <th>donchian_signal</th>\n",
       "      <th>macd</th>\n",
       "      <th>macd_signal</th>\n",
       "      <th>engulfing</th>\n",
       "      <th>hammer</th>\n",
       "      <th>invertedhammer</th>\n",
       "      <th>piercing</th>\n",
       "      <th>morningstar</th>\n",
       "      <th>3whitesoldiers</th>\n",
       "      <th>price_increased</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-07-31 09:25:00</th>\n",
       "      <td>426.0</td>\n",
       "      <td>426.49</td>\n",
       "      <td>425.61</td>\n",
       "      <td>426.49</td>\n",
       "      <td>34.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>427.44555899434135</td>\n",
       "      <td>426.5935427589915</td>\n",
       "      <td>0</td>\n",
       "      <td>51.06728678705997</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.053799168970045275</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-31 09:26:00</th>\n",
       "      <td>426.4</td>\n",
       "      <td>426.5</td>\n",
       "      <td>425.91</td>\n",
       "      <td>425.91</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>427.44555899434135</td>\n",
       "      <td>426.5933431977994</td>\n",
       "      <td>0</td>\n",
       "      <td>38.07641633728719</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0012007782475225437</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-31 09:27:00</th>\n",
       "      <td>426.5</td>\n",
       "      <td>427.34</td>\n",
       "      <td>425.91</td>\n",
       "      <td>426.55</td>\n",
       "      <td>238.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>427.44555899434135</td>\n",
       "      <td>426.59338621020436</td>\n",
       "      <td>0</td>\n",
       "      <td>50.73965218892885</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05530562698560017</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-31 09:28:00</th>\n",
       "      <td>426.61</td>\n",
       "      <td>426.85</td>\n",
       "      <td>426.41</td>\n",
       "      <td>426.85</td>\n",
       "      <td>103.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>427.44555899434135</td>\n",
       "      <td>426.5936928012562</td>\n",
       "      <td>1</td>\n",
       "      <td>50.78458505617963</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.09156435883502219</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-31 09:29:00</th>\n",
       "      <td>426.8</td>\n",
       "      <td>427.31</td>\n",
       "      <td>425.923</td>\n",
       "      <td>426.94</td>\n",
       "      <td>790.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>427.44555899434135</td>\n",
       "      <td>426.59642840229367</td>\n",
       "      <td>1</td>\n",
       "      <td>67.8468894268523</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.09052608973400926</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-31 09:30:00</th>\n",
       "      <td>426.51</td>\n",
       "      <td>427.11</td>\n",
       "      <td>426.3</td>\n",
       "      <td>427.005</td>\n",
       "      <td>71732.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>427.44555899434135</td>\n",
       "      <td>426.7331120724046</td>\n",
       "      <td>1</td>\n",
       "      <td>76.39691714836324</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.07435733848868686</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      open   high     low   close  volume  supertrend_signal  \\\n",
       "date                                                                           \n",
       "2023-07-31 09:25:00  426.0 426.49  425.61  426.49    34.0                 -1   \n",
       "2023-07-31 09:26:00  426.4  426.5  425.91  425.91    15.0                 -1   \n",
       "2023-07-31 09:27:00  426.5 427.34  425.91  426.55   238.0                 -1   \n",
       "2023-07-31 09:28:00 426.61 426.85  426.41  426.85   103.0                 -1   \n",
       "2023-07-31 09:29:00  426.8 427.31 425.923  426.94   790.0                 -1   \n",
       "2023-07-31 09:30:00 426.51 427.11   426.3 427.005 71732.0                 -1   \n",
       "\n",
       "                            supertrend               vwap  vwap_signal  \\\n",
       "date                                                                     \n",
       "2023-07-31 09:25:00 427.44555899434135  426.5935427589915            0   \n",
       "2023-07-31 09:26:00 427.44555899434135  426.5933431977994            0   \n",
       "2023-07-31 09:27:00 427.44555899434135 426.59338621020436            0   \n",
       "2023-07-31 09:28:00 427.44555899434135  426.5936928012562            1   \n",
       "2023-07-31 09:29:00 427.44555899434135 426.59642840229367            1   \n",
       "2023-07-31 09:30:00 427.44555899434135  426.7331120724046            1   \n",
       "\n",
       "                              stoch_k  ...  donchian_signal  \\\n",
       "date                                   ...                    \n",
       "2023-07-31 09:25:00 51.06728678705997  ...                1   \n",
       "2023-07-31 09:26:00 38.07641633728719  ...                1   \n",
       "2023-07-31 09:27:00 50.73965218892885  ...                1   \n",
       "2023-07-31 09:28:00 50.78458505617963  ...                1   \n",
       "2023-07-31 09:29:00  67.8468894268523  ...                1   \n",
       "2023-07-31 09:30:00 76.39691714836324  ...                1   \n",
       "\n",
       "                                     macd  macd_signal  engulfing  hammer  \\\n",
       "date                                                                        \n",
       "2023-07-31 09:25:00  0.053799168970045275            1        0.0     0.0   \n",
       "2023-07-31 09:26:00 0.0012007782475225437            0        0.0     0.0   \n",
       "2023-07-31 09:27:00   0.05530562698560017            0        0.0     0.0   \n",
       "2023-07-31 09:28:00   0.09156435883502219            0        0.0     0.0   \n",
       "2023-07-31 09:29:00   0.09052608973400926            0        0.0     0.0   \n",
       "2023-07-31 09:30:00   0.07435733848868686            0        0.0     0.0   \n",
       "\n",
       "                     invertedhammer  piercing  morningstar  3whitesoldiers  \\\n",
       "date                                                                         \n",
       "2023-07-31 09:25:00             0.0       0.0          0.0             0.0   \n",
       "2023-07-31 09:26:00             0.0       0.0          0.0             0.0   \n",
       "2023-07-31 09:27:00             0.0       0.0          0.0             0.0   \n",
       "2023-07-31 09:28:00             0.0       0.0          0.0             0.0   \n",
       "2023-07-31 09:29:00             0.0       0.0          0.0             0.0   \n",
       "2023-07-31 09:30:00             0.0       0.0          0.0             0.0   \n",
       "\n",
       "                     price_increased  \n",
       "date                                  \n",
       "2023-07-31 09:25:00                1  \n",
       "2023-07-31 09:26:00                0  \n",
       "2023-07-31 09:27:00                1  \n",
       "2023-07-31 09:28:00                1  \n",
       "2023-07-31 09:29:00                1  \n",
       "2023-07-31 09:30:00                1  \n",
       "\n",
       "[6 rows x 31 columns]"
      ]
     },
     "execution_count": 1698,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pick random 5 rows\n",
    "random_x = df[1990:1995]\n",
    "# scaling the rows\n",
    "random_x_scaled = scaler.transform(random_x)\n",
    "random_x_scaled = random_x_scaled.reshape(1, 5, num_features)\n",
    "# reshape so we can feed it to numpy\n",
    "# (5, 31) -> (1, 5, 31)\n",
    "\n",
    "prediction = model.predict(random_x_scaled)\n",
    "# >0.5 = 1 = True else False so that 0.9952133 will be 1 etc\n",
    "prediction = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Print the predictions\n",
    "print(\"Predicted Labels:\", prediction)\n",
    "df[1990:1996]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1699,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: rsi_signal, Weight: 2.657886266708374\n",
      "Feature: volume, Weight: 2.5759963989257812\n",
      "Feature: engulfing, Weight: 2.5741710662841797\n",
      "Feature: invertedhammer, Weight: 2.4467780590057373\n",
      "Feature: rsi, Weight: 2.4382731914520264\n",
      "Feature: macd, Weight: 2.4347383975982666\n",
      "Feature: stoch_signal, Weight: 2.4232380390167236\n",
      "Feature: stoch_k, Weight: 2.3380470275878906\n",
      "Feature: donchian_signal, Weight: 2.321899652481079\n",
      "Feature: adx, Weight: 2.315385103225708\n"
     ]
    }
   ],
   "source": [
    "# getting the weights of the input layer\n",
    "input_weights = model.layers[0].get_weights()[0]\n",
    "\n",
    "# dictionary to store each weight to its column\n",
    "weights_by_column = dict(zip(X.columns.tolist(), input_weights.T))\n",
    "\n",
    "feature_weights = {feature: np.abs(weight).sum() for feature, weight in weights_by_column.items()}\n",
    "top_10_features = sorted(feature_weights, key=lambda x: feature_weights[x], reverse=True)[:10]\n",
    "for feature in top_10_features:\n",
    "    print(f\"Feature: {feature}, Weight: {feature_weights[feature]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINE TUNING NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1805,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.layers import AlphaDropout\n",
    "\n",
    "# this object has fit(), score() and predict()\n",
    "# the score will be opposite of MSE -> higher is better\n",
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[30], optimizer='sgd'): \n",
    "\n",
    "    model = keras.models.Sequential()\n",
    "    options = {'input_shape': input_shape,\n",
    "                'activation':'selu',\n",
    "                'kernel_initializer':'lecun_normal',\n",
    "                'kernel_regularizer' : keras.regularizers.l1(0.01)}\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(Dense(n_neurons, **options))\n",
    "    options = {} \n",
    "    model.add(AlphaDropout(rate=0.2))\n",
    "    model.add(Dense(1, activation='linear', **options, kernel_initializer='lecun_normal', kernel_regularizer = keras.regularizers.l1(0.01))) \n",
    "    model.compile(loss=\"mse\", optimizer=optimizer) \n",
    "    if optimizer == 'adam':\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "    elif optimizer == 'sgd':\n",
    "        optimizer = keras.optimizers.SGD(learning_rate, clipnorm=1) \n",
    "    model.compile(loss=\"mse\", optimizer=optimizer) \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1849,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17487, 30)\n",
      "(17487, 1)\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "df = df_main.copy()\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# normalizing\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(df)\n",
    "df_scaled = scaler.transform(df)\n",
    "\n",
    "# dividing X and y\n",
    "X = df.drop(columns=['close'])\n",
    "y = df[['close']]\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "num_features = X.shape[1]\n",
    "print(num_features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1854,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[427.7  , 427.82 , 427.55 , ...,   0.   ,   0.   ,   1.   ],\n",
       "       [427.7  , 427.7  , 427.53 , ...,   0.   ,   0.   ,   0.   ],\n",
       "       [427.4  , 427.49 , 427.   , ...,   0.   ,   0.   ,   0.   ],\n",
       "       ...,\n",
       "       [412.935, 412.935, 412.935, ...,   0.   ,   0.   ,   0.   ],\n",
       "       [412.935, 412.935, 412.935, ...,   0.   ,   0.   ,   0.   ],\n",
       "       [412.935, 412.935, 412.935, ...,   0.   ,   0.   ,   0.   ]])"
      ]
     },
     "execution_count": 1854,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1851,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1852,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    " 'n_hidden': [1, 2, 3, 4],\n",
    " 'n_neurons': np.arange(1, 100),\n",
    " 'learning_rate': reciprocal(3e-4, 3e-2),\n",
    " 'optimizer': ['adam', 'sgd'],  # Add optimizer as a hyperparameter\n",
    "}\n",
    "grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=10, cv=3, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1853,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Epoch 1/30\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 109803400.0000 - val_loss: 383615.2812\n",
      "Epoch 2/30\n",
      "473/473 [==============================] - 0s 1ms/step - loss: 354848.8438 - val_loss: 255390.0625\n",
      "Epoch 3/30\n",
      "473/473 [==============================] - 0s 1ms/step - loss: 100920.0234 - val_loss: 332520.1250\n",
      "Epoch 4/30\n",
      "473/473 [==============================] - 0s 979us/step - loss: 159026.7969 - val_loss: 261369.5156\n",
      "Epoch 5/30\n",
      "473/473 [==============================] - 0s 879us/step - loss: 61625.8047 - val_loss: 212236.4375\n",
      "Epoch 6/30\n",
      "473/473 [==============================] - 0s 918us/step - loss: 50733.0625 - val_loss: 181715.4688\n",
      "Epoch 7/30\n",
      "473/473 [==============================] - 0s 865us/step - loss: 47989.1055 - val_loss: 310515.3438\n",
      "Epoch 8/30\n",
      "473/473 [==============================] - 0s 890us/step - loss: 46874.5820 - val_loss: 173859.2500\n",
      "Epoch 9/30\n",
      "473/473 [==============================] - 0s 892us/step - loss: 46178.9141 - val_loss: 407229.5938\n",
      "Epoch 10/30\n",
      "473/473 [==============================] - 0s 909us/step - loss: 44696.3984 - val_loss: 326956.6875\n",
      "Epoch 11/30\n",
      "473/473 [==============================] - 0s 892us/step - loss: 45199.1055 - val_loss: 218213.9844\n",
      "Epoch 12/30\n",
      "473/473 [==============================] - 0s 892us/step - loss: 45990.9414 - val_loss: 212668.0156\n",
      "Epoch 13/30\n",
      "473/473 [==============================] - 0s 871us/step - loss: 45572.1953 - val_loss: 191348.5000\n",
      "Epoch 14/30\n",
      "473/473 [==============================] - 0s 862us/step - loss: 44281.0781 - val_loss: 229814.4531\n",
      "Epoch 15/30\n",
      "473/473 [==============================] - 0s 869us/step - loss: 43008.4492 - val_loss: 269517.2500\n",
      "Epoch 16/30\n",
      "473/473 [==============================] - 0s 972us/step - loss: 42055.4297 - val_loss: 335699.9062\n",
      "Epoch 17/30\n",
      "473/473 [==============================] - 0s 892us/step - loss: 42169.3047 - val_loss: 57095.6953\n",
      "Epoch 18/30\n",
      "473/473 [==============================] - 0s 865us/step - loss: 40902.5195 - val_loss: 118501.1250\n",
      "Epoch 19/30\n",
      "473/473 [==============================] - 0s 871us/step - loss: 33983.2578 - val_loss: 56337.7070\n",
      "Epoch 20/30\n",
      "473/473 [==============================] - 0s 869us/step - loss: 24590.2129 - val_loss: 1089067.8750\n",
      "Epoch 21/30\n",
      "473/473 [==============================] - 0s 867us/step - loss: 16752.1230 - val_loss: 105883.1562\n",
      "Epoch 22/30\n",
      "473/473 [==============================] - 0s 863us/step - loss: 11607.7148 - val_loss: 20102.4199\n",
      "Epoch 23/30\n",
      "473/473 [==============================] - 0s 853us/step - loss: 160812.3125 - val_loss: 219624.4219\n",
      "Epoch 24/30\n",
      "473/473 [==============================] - 0s 849us/step - loss: 39046.4531 - val_loss: 271971.7188\n",
      "Epoch 25/30\n",
      "473/473 [==============================] - 0s 833us/step - loss: 34561.9883 - val_loss: 79193.6250\n",
      "Epoch 26/30\n",
      "473/473 [==============================] - 0s 839us/step - loss: 23060.6504 - val_loss: 269922.4375\n",
      "Epoch 27/30\n",
      "473/473 [==============================] - 0s 840us/step - loss: 18395.7930 - val_loss: 11345.0791\n",
      "Epoch 28/30\n",
      "473/473 [==============================] - 0s 838us/step - loss: 17847.0312 - val_loss: 33159.5039\n",
      "Epoch 29/30\n",
      "473/473 [==============================] - 0s 841us/step - loss: 17067.5039 - val_loss: 108918.3047\n",
      "Epoch 30/30\n",
      "473/473 [==============================] - 0s 837us/step - loss: 14410.3252 - val_loss: 126413.4766\n",
      "263/263 [==============================] - 0s 531us/step - loss: 25456.2402\n",
      "[CV 1/3] END learning_rate=0.021629088102836282, n_hidden=4, n_neurons=88, optimizer=adam;, score=-25456.240 total time=  13.6s\n",
      "Epoch 1/30\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 34951072.0000 - val_loss: 158842.8438\n",
      "Epoch 2/30\n",
      "473/473 [==============================] - 0s 816us/step - loss: 99274.4688 - val_loss: 270430.6562\n",
      "Epoch 3/30\n",
      "473/473 [==============================] - 0s 819us/step - loss: 75610.4141 - val_loss: 148370.4062\n",
      "Epoch 4/30\n",
      "473/473 [==============================] - 0s 816us/step - loss: 57213.4531 - val_loss: 200668.3906\n",
      "Epoch 5/30\n",
      "473/473 [==============================] - 0s 814us/step - loss: 53879.8477 - val_loss: 299815.8750\n",
      "Epoch 6/30\n",
      "473/473 [==============================] - 0s 809us/step - loss: 50943.9883 - val_loss: 115349.3359\n",
      "Epoch 7/30\n",
      "473/473 [==============================] - 0s 809us/step - loss: 50062.9023 - val_loss: 219950.9531\n",
      "Epoch 8/30\n",
      "473/473 [==============================] - 0s 811us/step - loss: 53664.8320 - val_loss: 218686.6094\n",
      "Epoch 9/30\n",
      "473/473 [==============================] - 0s 809us/step - loss: 69397.4453 - val_loss: 43453.9883\n",
      "Epoch 10/30\n",
      "473/473 [==============================] - 0s 852us/step - loss: 51303.2148 - val_loss: 192678.9375\n",
      "Epoch 11/30\n",
      "473/473 [==============================] - 0s 826us/step - loss: 44182.0234 - val_loss: 37974.1367\n",
      "Epoch 12/30\n",
      "473/473 [==============================] - 0s 966us/step - loss: 43490.3750 - val_loss: 160155.8125\n",
      "Epoch 13/30\n",
      "473/473 [==============================] - 0s 827us/step - loss: 24349.4805 - val_loss: 131956.0781\n",
      "Epoch 14/30\n",
      "473/473 [==============================] - 0s 813us/step - loss: 28460.2051 - val_loss: 83250.3203\n",
      "Epoch 15/30\n",
      "473/473 [==============================] - 0s 813us/step - loss: 16471.5254 - val_loss: 152118.0625\n",
      "Epoch 16/30\n",
      "473/473 [==============================] - 0s 815us/step - loss: 14136.8584 - val_loss: 50014.9648\n",
      "Epoch 17/30\n",
      "473/473 [==============================] - 0s 811us/step - loss: 11974.9980 - val_loss: 31474.6309\n",
      "Epoch 18/30\n",
      "473/473 [==============================] - 0s 813us/step - loss: 13325.4434 - val_loss: 205366.4219\n",
      "Epoch 19/30\n",
      "473/473 [==============================] - 0s 813us/step - loss: 1808337.1250 - val_loss: 148244.0000\n",
      "Epoch 20/30\n",
      "473/473 [==============================] - 0s 816us/step - loss: 43157.3008 - val_loss: 79385.3203\n",
      "Epoch 21/30\n",
      "473/473 [==============================] - 0s 815us/step - loss: 20249.6191 - val_loss: 104326.8281\n",
      "Epoch 22/30\n",
      "473/473 [==============================] - 0s 817us/step - loss: 15634.5566 - val_loss: 64508.6094\n",
      "Epoch 23/30\n",
      "473/473 [==============================] - 0s 818us/step - loss: 12983.1191 - val_loss: 7527.1284\n",
      "Epoch 24/30\n",
      "473/473 [==============================] - 0s 817us/step - loss: 16984.9551 - val_loss: 130264.9766\n",
      "Epoch 25/30\n",
      "473/473 [==============================] - 0s 811us/step - loss: 14231.5947 - val_loss: 83231.3750\n",
      "Epoch 26/30\n",
      "473/473 [==============================] - 0s 826us/step - loss: 12293.9668 - val_loss: 77369.2344\n",
      "Epoch 27/30\n",
      "473/473 [==============================] - 0s 811us/step - loss: 12264.6748 - val_loss: 9937.5674\n",
      "Epoch 28/30\n",
      "473/473 [==============================] - 0s 818us/step - loss: 16836.8340 - val_loss: 105762.8281\n",
      "Epoch 29/30\n",
      "473/473 [==============================] - 0s 814us/step - loss: 10045.8525 - val_loss: 47176.1445\n",
      "Epoch 30/30\n",
      "473/473 [==============================] - 0s 825us/step - loss: 8852.3887 - val_loss: 6813.2588\n",
      "263/263 [==============================] - 0s 528us/step - loss: 12126.9863\n",
      "[CV 2/3] END learning_rate=0.021629088102836282, n_hidden=4, n_neurons=88, optimizer=adam;, score=-12126.986 total time=  12.6s\n",
      "Epoch 1/30\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 4003285.2500 - val_loss: 123964.9531\n",
      "Epoch 2/30\n",
      "473/473 [==============================] - 0s 832us/step - loss: 119642.0391 - val_loss: 46064.9297\n",
      "Epoch 3/30\n",
      "473/473 [==============================] - 0s 833us/step - loss: 78081.2109 - val_loss: 59445.5273\n",
      "Epoch 4/30\n",
      "473/473 [==============================] - 0s 830us/step - loss: 71181.4922 - val_loss: 183506.5625\n",
      "Epoch 5/30\n",
      "473/473 [==============================] - 0s 831us/step - loss: 63646.6992 - val_loss: 145310.2969\n",
      "Epoch 6/30\n",
      "473/473 [==============================] - 0s 898us/step - loss: 62410.7695 - val_loss: 61913.7070\n",
      "Epoch 7/30\n",
      "473/473 [==============================] - 0s 840us/step - loss: 53503.0195 - val_loss: 156893.2344\n",
      "Epoch 8/30\n",
      "473/473 [==============================] - 0s 840us/step - loss: 45720.1133 - val_loss: 63530.9141\n",
      "Epoch 9/30\n",
      "473/473 [==============================] - 0s 843us/step - loss: 35129.0195 - val_loss: 80021.8047\n",
      "Epoch 10/30\n",
      "473/473 [==============================] - 0s 852us/step - loss: 29446.0020 - val_loss: 501573.1875\n",
      "Epoch 11/30\n",
      "473/473 [==============================] - 0s 902us/step - loss: 47873.3984 - val_loss: 32000.9609\n",
      "Epoch 12/30\n",
      "473/473 [==============================] - 0s 861us/step - loss: 24619.5234 - val_loss: 24433.3477\n",
      "Epoch 13/30\n",
      "473/473 [==============================] - 0s 899us/step - loss: 18693.9980 - val_loss: 9227.6680\n",
      "Epoch 14/30\n",
      "473/473 [==============================] - 0s 837us/step - loss: 12063.7646 - val_loss: 48170.9336\n",
      "Epoch 15/30\n",
      "473/473 [==============================] - 0s 837us/step - loss: 79270.9531 - val_loss: 10831.7119\n",
      "Epoch 16/30\n",
      "473/473 [==============================] - 0s 922us/step - loss: 40326.6172 - val_loss: 30404.7031\n",
      "Epoch 17/30\n",
      "473/473 [==============================] - 0s 849us/step - loss: 21851.2070 - val_loss: 44596.7539\n",
      "Epoch 18/30\n",
      "473/473 [==============================] - 0s 991us/step - loss: 27484.2051 - val_loss: 36543.0898\n",
      "Epoch 19/30\n",
      "473/473 [==============================] - 0s 917us/step - loss: 12506.4121 - val_loss: 66371.4688\n",
      "Epoch 20/30\n",
      "473/473 [==============================] - 0s 870us/step - loss: 10814.2910 - val_loss: 43229.1523\n",
      "Epoch 21/30\n",
      "473/473 [==============================] - 0s 851us/step - loss: 9244.8174 - val_loss: 15937.7461\n",
      "Epoch 22/30\n",
      "473/473 [==============================] - 0s 854us/step - loss: 8577.2129 - val_loss: 34614.8047\n",
      "Epoch 23/30\n",
      "473/473 [==============================] - 0s 847us/step - loss: 20042.7285 - val_loss: 6082.3579\n",
      "Epoch 24/30\n",
      "473/473 [==============================] - 0s 847us/step - loss: 12019.7793 - val_loss: 2242.9331\n",
      "Epoch 25/30\n",
      "473/473 [==============================] - 0s 856us/step - loss: 7814.7314 - val_loss: 541.5906\n",
      "Epoch 26/30\n",
      "473/473 [==============================] - 0s 858us/step - loss: 7552.1519 - val_loss: 37563.2812\n",
      "Epoch 27/30\n",
      "473/473 [==============================] - 0s 862us/step - loss: 6656.2446 - val_loss: 53591.1797\n",
      "Epoch 28/30\n",
      "473/473 [==============================] - 0s 838us/step - loss: 5594.5327 - val_loss: 10830.7549\n",
      "Epoch 29/30\n",
      "473/473 [==============================] - 0s 846us/step - loss: 6318.9487 - val_loss: 34850.2812\n",
      "Epoch 30/30\n",
      "473/473 [==============================] - 0s 847us/step - loss: 5012.0508 - val_loss: 34691.2148\n",
      "263/263 [==============================] - 0s 531us/step - loss: 111673.0859\n",
      "[CV 3/3] END learning_rate=0.021629088102836282, n_hidden=4, n_neurons=88, optimizer=adam;, score=-111673.086 total time=  13.1s\n",
      "Epoch 1/30\n",
      "473/473 [==============================] - 1s 819us/step - loss: 6994471424.0000 - val_loss: 732486912.0000\n",
      "Epoch 2/30\n",
      "473/473 [==============================] - 0s 692us/step - loss: 3075055104.0000 - val_loss: 144296544.0000\n",
      "Epoch 3/30\n",
      "473/473 [==============================] - 0s 782us/step - loss: 1372668928.0000 - val_loss: 121308528.0000\n",
      "Epoch 4/30\n",
      "473/473 [==============================] - 0s 757us/step - loss: 527747456.0000 - val_loss: 22251290.0000\n",
      "Epoch 5/30\n",
      "473/473 [==============================] - 0s 742us/step - loss: 190181712.0000 - val_loss: 91473744.0000\n",
      "Epoch 6/30\n",
      "473/473 [==============================] - 0s 746us/step - loss: 65932124.0000 - val_loss: 1463353.8750\n",
      "Epoch 7/30\n",
      "473/473 [==============================] - 0s 729us/step - loss: 28078980.0000 - val_loss: 4697251.5000\n",
      "Epoch 8/30\n",
      "473/473 [==============================] - 0s 761us/step - loss: 18833824.0000 - val_loss: 461808.5625\n",
      "Epoch 9/30\n",
      "473/473 [==============================] - 0s 754us/step - loss: 16836622.0000 - val_loss: 2017552.8750\n",
      "Epoch 10/30\n",
      "473/473 [==============================] - 0s 757us/step - loss: 14439025.0000 - val_loss: 4263036.5000\n",
      "Epoch 11/30\n",
      "473/473 [==============================] - 0s 714us/step - loss: 11791737.0000 - val_loss: 154945.9531\n",
      "Epoch 12/30\n",
      "473/473 [==============================] - 0s 746us/step - loss: 9578120.0000 - val_loss: 1769569.5000\n",
      "Epoch 13/30\n",
      "473/473 [==============================] - 0s 751us/step - loss: 8436701.0000 - val_loss: 2269565.0000\n",
      "Epoch 14/30\n",
      "473/473 [==============================] - 0s 689us/step - loss: 7133839.0000 - val_loss: 3710493.7500\n",
      "Epoch 15/30\n",
      "473/473 [==============================] - 0s 660us/step - loss: 5967554.0000 - val_loss: 1357843.6250\n",
      "Epoch 16/30\n",
      "473/473 [==============================] - 0s 666us/step - loss: 5197457.5000 - val_loss: 2200862.5000\n",
      "Epoch 17/30\n",
      "473/473 [==============================] - 0s 668us/step - loss: 4370106.5000 - val_loss: 2303890.0000\n",
      "Epoch 18/30\n",
      "473/473 [==============================] - 0s 652us/step - loss: 4322620.5000 - val_loss: 3339108.0000\n",
      "Epoch 19/30\n",
      "473/473 [==============================] - 0s 661us/step - loss: 3108046.0000 - val_loss: 3980728.2500\n",
      "Epoch 20/30\n",
      "473/473 [==============================] - 0s 660us/step - loss: 3196017.0000 - val_loss: 2578113.2500\n",
      "Epoch 21/30\n",
      "473/473 [==============================] - 0s 701us/step - loss: 2744564.7500 - val_loss: 331419.6562\n",
      "263/263 [==============================] - 0s 489us/step - loss: 3489359.7500\n",
      "[CV 1/3] END learning_rate=0.0007475736846458284, n_hidden=1, n_neurons=88, optimizer=sgd;, score=-3489359.750 total time=   7.6s\n",
      "Epoch 1/30\n",
      "473/473 [==============================] - 1s 817us/step - loss: 22828656640.0000 - val_loss: 5309682.0000\n",
      "Epoch 2/30\n",
      "473/473 [==============================] - 0s 671us/step - loss: 4591044608.0000 - val_loss: 56906528.0000\n",
      "Epoch 3/30\n",
      "473/473 [==============================] - 0s 737us/step - loss: 2170046976.0000 - val_loss: 11891522.0000\n",
      "Epoch 4/30\n",
      "473/473 [==============================] - 0s 693us/step - loss: 957795200.0000 - val_loss: 158087200.0000\n",
      "Epoch 5/30\n",
      "473/473 [==============================] - 0s 666us/step - loss: 420206016.0000 - val_loss: 17575072.0000\n",
      "Epoch 6/30\n",
      "473/473 [==============================] - 0s 671us/step - loss: 214111568.0000 - val_loss: 1838986.2500\n",
      "Epoch 7/30\n",
      "473/473 [==============================] - 0s 666us/step - loss: 116582184.0000 - val_loss: 9700110.0000\n",
      "Epoch 8/30\n",
      "473/473 [==============================] - 0s 676us/step - loss: 72611848.0000 - val_loss: 23650708.0000\n",
      "Epoch 9/30\n",
      "473/473 [==============================] - 0s 663us/step - loss: 45472644.0000 - val_loss: 2828827.0000\n",
      "Epoch 10/30\n",
      "473/473 [==============================] - 0s 691us/step - loss: 27210930.0000 - val_loss: 846946.9375\n",
      "Epoch 11/30\n",
      "473/473 [==============================] - 0s 753us/step - loss: 18242324.0000 - val_loss: 5776973.0000\n",
      "Epoch 12/30\n",
      "473/473 [==============================] - 0s 739us/step - loss: 12061863.0000 - val_loss: 831395.1250\n",
      "Epoch 13/30\n",
      "473/473 [==============================] - 0s 795us/step - loss: 7569947.5000 - val_loss: 721485.4375\n",
      "Epoch 14/30\n",
      "473/473 [==============================] - 0s 732us/step - loss: 5279549.0000 - val_loss: 3719043.7500\n",
      "Epoch 15/30\n",
      "473/473 [==============================] - 0s 677us/step - loss: 3869821.2500 - val_loss: 4923621.0000\n",
      "Epoch 16/30\n",
      "473/473 [==============================] - 0s 727us/step - loss: 2750514.0000 - val_loss: 4315067.5000\n",
      "Epoch 17/30\n",
      "473/473 [==============================] - 0s 672us/step - loss: 1997148.7500 - val_loss: 64238.2383\n",
      "Epoch 18/30\n",
      "473/473 [==============================] - 0s 664us/step - loss: 1493624.7500 - val_loss: 1072356.0000\n",
      "Epoch 19/30\n",
      "473/473 [==============================] - 0s 694us/step - loss: 1322216.1250 - val_loss: 960623.4375\n",
      "Epoch 20/30\n",
      "473/473 [==============================] - 0s 686us/step - loss: 1095380.0000 - val_loss: 490358.5938\n",
      "Epoch 21/30\n",
      "473/473 [==============================] - 0s 692us/step - loss: 998399.3750 - val_loss: 1827965.3750\n",
      "Epoch 22/30\n",
      "473/473 [==============================] - 0s 737us/step - loss: 816721.5625 - val_loss: 296249.1562\n",
      "Epoch 23/30\n",
      "473/473 [==============================] - 0s 667us/step - loss: 1006726.9375 - val_loss: 663788.5000\n",
      "Epoch 24/30\n",
      "473/473 [==============================] - 0s 738us/step - loss: 876620.1250 - val_loss: 17931974.0000\n",
      "Epoch 25/30\n",
      "473/473 [==============================] - 0s 690us/step - loss: 1035938.4375 - val_loss: 920436.4375\n",
      "Epoch 26/30\n",
      "473/473 [==============================] - 0s 683us/step - loss: 1109414.2500 - val_loss: 60833.1992\n",
      "Epoch 27/30\n",
      "473/473 [==============================] - 0s 670us/step - loss: 601284.0000 - val_loss: 2335135.7500\n",
      "Epoch 28/30\n",
      "473/473 [==============================] - 0s 676us/step - loss: 737004.0625 - val_loss: 5799296.0000\n",
      "Epoch 29/30\n",
      "473/473 [==============================] - 0s 714us/step - loss: 993080.3125 - val_loss: 30412510.0000\n",
      "Epoch 30/30\n",
      "473/473 [==============================] - 0s 684us/step - loss: 893139.3125 - val_loss: 7705009.0000\n",
      "263/263 [==============================] - 0s 575us/step - loss: 272855.0000\n",
      "[CV 2/3] END learning_rate=0.0007475736846458284, n_hidden=1, n_neurons=88, optimizer=sgd;, score=-272855.000 total time=  10.5s\n",
      "Epoch 1/30\n",
      "473/473 [==============================] - 1s 863us/step - loss: 1234053248.0000 - val_loss: 19094286.0000\n",
      "Epoch 2/30\n",
      "473/473 [==============================] - 0s 691us/step - loss: 610219456.0000 - val_loss: 67507840.0000\n",
      "Epoch 3/30\n",
      "473/473 [==============================] - 0s 701us/step - loss: 393715968.0000 - val_loss: 7265474.0000\n",
      "Epoch 4/30\n",
      "473/473 [==============================] - 0s 699us/step - loss: 275025248.0000 - val_loss: 4389571.5000\n",
      "Epoch 5/30\n",
      "473/473 [==============================] - 0s 787us/step - loss: 160137216.0000 - val_loss: 7195378.0000\n",
      "Epoch 6/30\n",
      "473/473 [==============================] - 0s 673us/step - loss: 105285968.0000 - val_loss: 1108351.1250\n",
      "Epoch 7/30\n",
      "473/473 [==============================] - 0s 659us/step - loss: 64654236.0000 - val_loss: 561647.1250\n",
      "Epoch 8/30\n",
      "473/473 [==============================] - 0s 660us/step - loss: 39140100.0000 - val_loss: 3531200.7500\n",
      "Epoch 9/30\n",
      "473/473 [==============================] - 0s 679us/step - loss: 22823650.0000 - val_loss: 133694.7812\n",
      "Epoch 10/30\n",
      "473/473 [==============================] - 0s 667us/step - loss: 12885615.0000 - val_loss: 1740359.3750\n",
      "Epoch 11/30\n",
      "473/473 [==============================] - 0s 674us/step - loss: 7556328.5000 - val_loss: 2572556.2500\n",
      "Epoch 12/30\n",
      "473/473 [==============================] - 0s 677us/step - loss: 3873594.0000 - val_loss: 3425747.5000\n",
      "Epoch 13/30\n",
      "473/473 [==============================] - 0s 728us/step - loss: 2805499.5000 - val_loss: 910622.2500\n",
      "Epoch 14/30\n",
      "473/473 [==============================] - 0s 677us/step - loss: 1918375.0000 - val_loss: 1221625.6250\n",
      "Epoch 15/30\n",
      "473/473 [==============================] - 0s 674us/step - loss: 1092889.2500 - val_loss: 54396.7070\n",
      "Epoch 16/30\n",
      "473/473 [==============================] - 0s 672us/step - loss: 1099054.7500 - val_loss: 4039003.5000\n",
      "Epoch 17/30\n",
      "473/473 [==============================] - 0s 669us/step - loss: 1199050.7500 - val_loss: 418808.6875\n",
      "Epoch 18/30\n",
      "473/473 [==============================] - 0s 672us/step - loss: 936529.4375 - val_loss: 251510.9219\n",
      "Epoch 19/30\n",
      "473/473 [==============================] - 0s 670us/step - loss: 863617.3750 - val_loss: 477580.7188\n",
      "Epoch 20/30\n",
      "473/473 [==============================] - 0s 671us/step - loss: 615691.6250 - val_loss: 1303675.1250\n",
      "Epoch 21/30\n",
      "473/473 [==============================] - 0s 670us/step - loss: 536389.0000 - val_loss: 611343.4375\n",
      "Epoch 22/30\n",
      "473/473 [==============================] - 0s 661us/step - loss: 906275.8750 - val_loss: 85212.0234\n",
      "Epoch 23/30\n",
      "473/473 [==============================] - 0s 675us/step - loss: 1441021.5000 - val_loss: 34297.0859\n",
      "Epoch 24/30\n",
      "473/473 [==============================] - 0s 671us/step - loss: 711433.5000 - val_loss: 3591138.2500\n",
      "Epoch 25/30\n",
      "473/473 [==============================] - 0s 662us/step - loss: 586370.5625 - val_loss: 457942.2812\n",
      "Epoch 26/30\n",
      "473/473 [==============================] - 0s 713us/step - loss: 998571.4375 - val_loss: 310573.4375\n",
      "Epoch 27/30\n",
      "473/473 [==============================] - 0s 671us/step - loss: 492338.4375 - val_loss: 88342.2422\n",
      "Epoch 28/30\n",
      "473/473 [==============================] - 0s 668us/step - loss: 682588.1875 - val_loss: 50464.4414\n",
      "Epoch 29/30\n",
      "473/473 [==============================] - 0s 665us/step - loss: 666851.6250 - val_loss: 378016.5312\n",
      "Epoch 30/30\n",
      "473/473 [==============================] - 0s 670us/step - loss: 319902.6250 - val_loss: 36605.7031\n",
      "263/263 [==============================] - 0s 473us/step - loss: 264460.3125\n",
      "[CV 3/3] END learning_rate=0.0007475736846458284, n_hidden=1, n_neurons=88, optimizer=sgd;, score=-264460.312 total time=  10.2s\n",
      "Epoch 1/30\n",
      "473/473 [==============================] - 1s 941us/step - loss: 20790802.0000 - val_loss: 333994.5938\n",
      "Epoch 2/30\n",
      "473/473 [==============================] - 0s 786us/step - loss: 170328.6406 - val_loss: 320734.8125\n",
      "Epoch 3/30\n",
      "473/473 [==============================] - 0s 749us/step - loss: 141242.8438 - val_loss: 424296.6875\n",
      "Epoch 4/30\n",
      "473/473 [==============================] - 0s 726us/step - loss: 80913.9766 - val_loss: 284883.6250\n",
      "Epoch 5/30\n",
      "473/473 [==============================] - 0s 739us/step - loss: 62167.1211 - val_loss: 208800.8750\n",
      "Epoch 6/30\n",
      "473/473 [==============================] - 0s 917us/step - loss: 55751.5352 - val_loss: 326127.3125\n",
      "Epoch 7/30\n",
      "473/473 [==============================] - 0s 750us/step - loss: 57686.1641 - val_loss: 278501.6562\n",
      "Epoch 8/30\n",
      "473/473 [==============================] - 0s 869us/step - loss: 71896.3438 - val_loss: 69332.1719\n",
      "Epoch 9/30\n",
      "473/473 [==============================] - 0s 877us/step - loss: 49489.8281 - val_loss: 278719.6562\n",
      "Epoch 10/30\n",
      "473/473 [==============================] - 0s 862us/step - loss: 46989.3398 - val_loss: 410482.6875\n",
      "Epoch 11/30\n",
      "473/473 [==============================] - 0s 799us/step - loss: 51965.2734 - val_loss: 125109.8828\n",
      "Epoch 12/30\n",
      "473/473 [==============================] - 0s 823us/step - loss: 45241.4883 - val_loss: 146705.1094\n",
      "Epoch 13/30\n",
      "473/473 [==============================] - 0s 932us/step - loss: 43208.1602 - val_loss: 597967.1875\n",
      "Epoch 14/30\n",
      "473/473 [==============================] - 0s 778us/step - loss: 37433.1797 - val_loss: 82254.5703\n",
      "Epoch 15/30\n",
      "473/473 [==============================] - 0s 862us/step - loss: 35371.0820 - val_loss: 7380.8638\n",
      "Epoch 16/30\n",
      "473/473 [==============================] - 0s 799us/step - loss: 30072.4961 - val_loss: 2799.6750\n",
      "Epoch 17/30\n",
      "473/473 [==============================] - 0s 820us/step - loss: 73989.8203 - val_loss: 156125.4375\n",
      "Epoch 18/30\n",
      "473/473 [==============================] - 0s 795us/step - loss: 62403.3477 - val_loss: 97649.1484\n",
      "Epoch 19/30\n",
      "473/473 [==============================] - 0s 768us/step - loss: 60887.0312 - val_loss: 126259.3438\n",
      "Epoch 20/30\n",
      "473/473 [==============================] - 0s 757us/step - loss: 59539.4336 - val_loss: 58084.5430\n",
      "Epoch 21/30\n",
      "473/473 [==============================] - 0s 745us/step - loss: 53418.4531 - val_loss: 8220.6348\n",
      "Epoch 22/30\n",
      "473/473 [==============================] - 0s 758us/step - loss: 43307.4805 - val_loss: 108382.6875\n",
      "Epoch 23/30\n",
      "473/473 [==============================] - 0s 744us/step - loss: 33289.2227 - val_loss: 134638.7812\n",
      "Epoch 24/30\n",
      "473/473 [==============================] - 0s 744us/step - loss: 26112.8789 - val_loss: 140265.0312\n",
      "Epoch 25/30\n",
      "473/473 [==============================] - 0s 789us/step - loss: 20546.8203 - val_loss: 44967.9141\n",
      "Epoch 26/30\n",
      "473/473 [==============================] - 0s 757us/step - loss: 18092.3125 - val_loss: 12400.4150\n",
      "263/263 [==============================] - 0s 494us/step - loss: 152510.0312\n",
      "[CV 1/3] END learning_rate=0.0004076564878056142, n_hidden=3, n_neurons=59, optimizer=adam;, score=-152510.031 total time=  10.6s\n",
      "Epoch 1/30\n",
      "473/473 [==============================] - 1s 920us/step - loss: 70700640.0000 - val_loss: 498879.4688\n",
      "Epoch 2/30\n",
      "473/473 [==============================] - 0s 778us/step - loss: 212222.1250 - val_loss: 330417.9062\n",
      "Epoch 3/30\n",
      "473/473 [==============================] - 0s 757us/step - loss: 129513.0859 - val_loss: 193915.2500\n",
      "Epoch 4/30\n",
      "473/473 [==============================] - 0s 771us/step - loss: 140804.4844 - val_loss: 234624.1250\n",
      "Epoch 5/30\n",
      "473/473 [==============================] - 0s 973us/step - loss: 76173.3125 - val_loss: 65684.9844\n",
      "Epoch 6/30\n",
      "473/473 [==============================] - 0s 785us/step - loss: 119741.8984 - val_loss: 146596.6406\n",
      "Epoch 7/30\n",
      "473/473 [==============================] - 0s 820us/step - loss: 95636.0781 - val_loss: 251528.5312\n",
      "Epoch 8/30\n",
      "473/473 [==============================] - 0s 748us/step - loss: 38246.2070 - val_loss: 117526.1016\n",
      "Epoch 9/30\n",
      "473/473 [==============================] - 0s 750us/step - loss: 28950.6875 - val_loss: 129035.7500\n",
      "Epoch 10/30\n",
      "473/473 [==============================] - 0s 747us/step - loss: 23892.8613 - val_loss: 166526.4219\n",
      "Epoch 11/30\n",
      "473/473 [==============================] - 0s 745us/step - loss: 53458.2773 - val_loss: 127767.1953\n",
      "Epoch 12/30\n",
      "473/473 [==============================] - 0s 746us/step - loss: 27462.2812 - val_loss: 24821.9648\n",
      "Epoch 13/30\n",
      "473/473 [==============================] - 0s 745us/step - loss: 21285.3047 - val_loss: 236954.2344\n",
      "Epoch 14/30\n",
      "473/473 [==============================] - 0s 741us/step - loss: 17649.3242 - val_loss: 212086.1094\n",
      "Epoch 15/30\n",
      "473/473 [==============================] - 0s 744us/step - loss: 17948.0000 - val_loss: 70842.2266\n",
      "Epoch 16/30\n",
      "473/473 [==============================] - 0s 800us/step - loss: 19564.1719 - val_loss: 145547.7344\n",
      "Epoch 17/30\n",
      "473/473 [==============================] - 0s 757us/step - loss: 16737.2266 - val_loss: 208787.5469\n",
      "Epoch 18/30\n",
      "473/473 [==============================] - 0s 742us/step - loss: 19481.1621 - val_loss: 279487.0625\n",
      "Epoch 19/30\n",
      "473/473 [==============================] - 0s 744us/step - loss: 16272.5439 - val_loss: 67213.4141\n",
      "Epoch 20/30\n",
      "473/473 [==============================] - 0s 742us/step - loss: 15124.2705 - val_loss: 257829.0625\n",
      "Epoch 21/30\n",
      "473/473 [==============================] - 0s 744us/step - loss: 70542.5859 - val_loss: 108462.0781\n",
      "Epoch 22/30\n",
      "473/473 [==============================] - 0s 740us/step - loss: 40369.3516 - val_loss: 203896.3281\n",
      "263/263 [==============================] - 0s 498us/step - loss: 16820.2754\n",
      "[CV 2/3] END learning_rate=0.0004076564878056142, n_hidden=3, n_neurons=59, optimizer=adam;, score=-16820.275 total time=   8.7s\n",
      "Epoch 1/30\n",
      "473/473 [==============================] - 1s 983us/step - loss: 23093540.0000 - val_loss: 157270.2188\n",
      "Epoch 2/30\n",
      "473/473 [==============================] - 0s 746us/step - loss: 111835.6797 - val_loss: 276450.7500\n",
      "Epoch 3/30\n",
      "473/473 [==============================] - 0s 774us/step - loss: 736538.5000 - val_loss: 360095.9688\n",
      "Epoch 4/30\n",
      "473/473 [==============================] - 0s 748us/step - loss: 140312.2188 - val_loss: 250698.2500\n",
      "Epoch 5/30\n",
      "473/473 [==============================] - 0s 766us/step - loss: 82857.9844 - val_loss: 93696.7188\n",
      "Epoch 6/30\n",
      "473/473 [==============================] - 0s 760us/step - loss: 72661.0156 - val_loss: 216408.5469\n",
      "Epoch 7/30\n",
      "473/473 [==============================] - 0s 876us/step - loss: 82077.4062 - val_loss: 71765.5938\n",
      "Epoch 8/30\n",
      "473/473 [==============================] - 0s 762us/step - loss: 61674.2617 - val_loss: 75191.1328\n",
      "Epoch 9/30\n",
      "473/473 [==============================] - 0s 820us/step - loss: 52799.0078 - val_loss: 112695.0469\n",
      "Epoch 10/30\n",
      "473/473 [==============================] - 0s 756us/step - loss: 51858.3633 - val_loss: 69377.0078\n",
      "Epoch 11/30\n",
      "473/473 [==============================] - 0s 749us/step - loss: 42986.6055 - val_loss: 113813.3906\n",
      "Epoch 12/30\n",
      "473/473 [==============================] - 0s 757us/step - loss: 29980.1094 - val_loss: 264246.5625\n",
      "Epoch 13/30\n",
      "473/473 [==============================] - 0s 759us/step - loss: 26851.3945 - val_loss: 37009.6875\n",
      "Epoch 14/30\n",
      "473/473 [==============================] - 0s 762us/step - loss: 23605.8145 - val_loss: 109183.4141\n",
      "Epoch 15/30\n",
      "473/473 [==============================] - 0s 752us/step - loss: 20363.6855 - val_loss: 114966.4062\n",
      "Epoch 16/30\n",
      "473/473 [==============================] - 0s 761us/step - loss: 13264.2109 - val_loss: 17496.2734\n",
      "Epoch 17/30\n",
      "473/473 [==============================] - 0s 796us/step - loss: 11920.9531 - val_loss: 96643.8594\n",
      "Epoch 18/30\n",
      "473/473 [==============================] - 0s 786us/step - loss: 11079.8525 - val_loss: 1893.3174\n",
      "Epoch 19/30\n",
      "473/473 [==============================] - 0s 780us/step - loss: 14679.8818 - val_loss: 48444.7891\n",
      "Epoch 20/30\n",
      "473/473 [==============================] - 0s 797us/step - loss: 10963.5098 - val_loss: 61919.6992\n",
      "Epoch 21/30\n",
      "473/473 [==============================] - 0s 768us/step - loss: 10378.9404 - val_loss: 249518.8594\n",
      "Epoch 22/30\n",
      "473/473 [==============================] - 0s 765us/step - loss: 9151.5459 - val_loss: 8829.4326\n",
      "Epoch 23/30\n",
      "473/473 [==============================] - 0s 815us/step - loss: 7006.0269 - val_loss: 46273.2266\n",
      "Epoch 24/30\n",
      "473/473 [==============================] - 0s 805us/step - loss: 6791.2354 - val_loss: 17640.4961\n",
      "Epoch 25/30\n",
      "473/473 [==============================] - 0s 818us/step - loss: 7670.5386 - val_loss: 58174.8750\n",
      "Epoch 26/30\n",
      "473/473 [==============================] - 0s 777us/step - loss: 6760.2354 - val_loss: 5492.6660\n",
      "Epoch 27/30\n",
      "473/473 [==============================] - 0s 765us/step - loss: 6725.2368 - val_loss: 3560.1929\n",
      "Epoch 28/30\n",
      "473/473 [==============================] - 0s 781us/step - loss: 7043.2769 - val_loss: 14677.8018\n",
      "263/263 [==============================] - 0s 513us/step - loss: 30694.0723\n",
      "[CV 3/3] END learning_rate=0.0004076564878056142, n_hidden=3, n_neurons=59, optimizer=adam;, score=-30694.072 total time=  11.1s\n",
      "Epoch 1/30\n",
      "473/473 [==============================] - 1s 975us/step - loss: 327961600.0000 - val_loss: 192758.9688\n",
      "Epoch 2/30\n",
      "473/473 [==============================] - 0s 931us/step - loss: 765350.0625 - val_loss: 316364.1250\n",
      "Epoch 3/30\n",
      "473/473 [==============================] - 0s 766us/step - loss: 293084.0625 - val_loss: 324090.5000\n",
      "Epoch 4/30\n",
      "473/473 [==============================] - 0s 815us/step - loss: 295191.2500 - val_loss: 219107.2500\n",
      "Epoch 5/30\n",
      "473/473 [==============================] - 0s 760us/step - loss: 93928.0000 - val_loss: 323267.0938\n",
      "Epoch 6/30\n",
      "473/473 [==============================] - 0s 764us/step - loss: 59445.3242 - val_loss: 348600.7500\n",
      "Epoch 7/30\n",
      "473/473 [==============================] - 0s 766us/step - loss: 76734.5859 - val_loss: 169586.8594\n",
      "Epoch 8/30\n",
      "473/473 [==============================] - 0s 755us/step - loss: 79058.3203 - val_loss: 188395.8906\n",
      "Epoch 9/30\n",
      "473/473 [==============================] - 0s 764us/step - loss: 73795.6562 - val_loss: 241423.5312\n",
      "Epoch 10/30\n",
      "473/473 [==============================] - 0s 764us/step - loss: 53976.2148 - val_loss: 230664.7500\n",
      "Epoch 11/30\n",
      "473/473 [==============================] - 0s 827us/step - loss: 75806.4141 - val_loss: 161653.1719\n",
      "Epoch 12/30\n",
      "473/473 [==============================] - 0s 764us/step - loss: 61709.4141 - val_loss: 177414.7500\n",
      "Epoch 13/30\n",
      "473/473 [==============================] - 0s 767us/step - loss: 45454.6250 - val_loss: 206459.2812\n",
      "Epoch 14/30\n",
      "473/473 [==============================] - 0s 761us/step - loss: 44353.5039 - val_loss: 122969.7266\n",
      "Epoch 15/30\n",
      "473/473 [==============================] - 0s 763us/step - loss: 43368.9883 - val_loss: 215098.2656\n",
      "Epoch 16/30\n",
      "473/473 [==============================] - 0s 760us/step - loss: 42026.4883 - val_loss: 411837.0938\n",
      "Epoch 17/30\n",
      "473/473 [==============================] - 0s 759us/step - loss: 40993.3203 - val_loss: 268055.6250\n",
      "Epoch 18/30\n",
      "473/473 [==============================] - 0s 806us/step - loss: 39251.1367 - val_loss: 283960.5312\n",
      "Epoch 19/30\n",
      "473/473 [==============================] - 0s 748us/step - loss: 37828.3945 - val_loss: 110819.5938\n",
      "Epoch 20/30\n",
      "473/473 [==============================] - 0s 763us/step - loss: 36754.4023 - val_loss: 181721.5312\n",
      "Epoch 21/30\n",
      "473/473 [==============================] - 0s 758us/step - loss: 85147.2109 - val_loss: 265515.7812\n",
      "Epoch 22/30\n",
      "473/473 [==============================] - 0s 760us/step - loss: 37513.6680 - val_loss: 156599.6875\n",
      "Epoch 23/30\n",
      "473/473 [==============================] - 0s 820us/step - loss: 35653.3555 - val_loss: 555801.1250\n",
      "Epoch 24/30\n",
      "473/473 [==============================] - 0s 853us/step - loss: 30618.6055 - val_loss: 104063.4297\n",
      "Epoch 25/30\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 26313.9941 - val_loss: 107614.0859\n",
      "Epoch 26/30\n",
      "473/473 [==============================] - 0s 779us/step - loss: 22156.3535 - val_loss: 69358.1094\n",
      "Epoch 27/30\n",
      "473/473 [==============================] - 0s 772us/step - loss: 60939.9219 - val_loss: 2467.9521\n",
      "Epoch 28/30\n",
      "473/473 [==============================] - 0s 805us/step - loss: 31118.9258 - val_loss: 6782.9883\n",
      "Epoch 29/30\n",
      "473/473 [==============================] - 0s 773us/step - loss: 93814.3750 - val_loss: 5986.2280\n",
      "Epoch 30/30\n",
      "473/473 [==============================] - 0s 757us/step - loss: 25270.8281 - val_loss: 23655.8926\n",
      "263/263 [==============================] - 0s 523us/step - loss: 54006.5273\n",
      "[CV 1/3] END learning_rate=0.0014732302002701152, n_hidden=2, n_neurons=92, optimizer=adam;, score=-54006.527 total time=  12.0s\n",
      "Epoch 1/30\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 356243904.0000 - val_loss: 2432773.5000\n",
      "Epoch 2/30\n",
      "473/473 [==============================] - 0s 843us/step - loss: 1098485.3750 - val_loss: 2276667.0000\n",
      "Epoch 3/30\n",
      "473/473 [==============================] - 0s 802us/step - loss: 354638.9062 - val_loss: 224219.0781\n",
      "Epoch 4/30\n",
      "473/473 [==============================] - 0s 766us/step - loss: 90399.6406 - val_loss: 171056.4531\n",
      "Epoch 5/30\n",
      "473/473 [==============================] - 0s 751us/step - loss: 75276.2188 - val_loss: 282451.9375\n",
      "Epoch 6/30\n",
      "473/473 [==============================] - 0s 754us/step - loss: 141934.0625 - val_loss: 304363.2188\n",
      "Epoch 7/30\n",
      "473/473 [==============================] - 0s 759us/step - loss: 133511.5000 - val_loss: 363405.5938\n",
      "Epoch 8/30\n",
      "473/473 [==============================] - 0s 811us/step - loss: 88151.7734 - val_loss: 268362.5000\n",
      "Epoch 9/30\n",
      "473/473 [==============================] - 0s 765us/step - loss: 72557.5000 - val_loss: 202463.0312\n",
      "Epoch 10/30\n",
      "473/473 [==============================] - 0s 763us/step - loss: 52605.1562 - val_loss: 206467.7812\n",
      "Epoch 11/30\n",
      "473/473 [==============================] - 0s 790us/step - loss: 45936.5508 - val_loss: 125706.1719\n",
      "Epoch 12/30\n",
      "473/473 [==============================] - 0s 757us/step - loss: 40158.0391 - val_loss: 273099.3750\n",
      "Epoch 13/30\n",
      "473/473 [==============================] - 0s 761us/step - loss: 39489.4727 - val_loss: 371594.5000\n",
      "Epoch 14/30\n",
      "473/473 [==============================] - 0s 996us/step - loss: 37187.0664 - val_loss: 99493.7734\n",
      "Epoch 15/30\n",
      "473/473 [==============================] - 0s 801us/step - loss: 41156.0234 - val_loss: 109558.0391\n",
      "Epoch 16/30\n",
      "473/473 [==============================] - 0s 795us/step - loss: 36102.3633 - val_loss: 99901.3516\n",
      "Epoch 17/30\n",
      "473/473 [==============================] - 0s 797us/step - loss: 33445.4609 - val_loss: 165810.6250\n",
      "Epoch 18/30\n",
      "473/473 [==============================] - 0s 773us/step - loss: 30403.5273 - val_loss: 73134.6484\n",
      "Epoch 19/30\n",
      "473/473 [==============================] - 0s 771us/step - loss: 27392.0703 - val_loss: 346435.0312\n",
      "Epoch 20/30\n",
      "473/473 [==============================] - 0s 830us/step - loss: 46198.1836 - val_loss: 129880.4609\n",
      "Epoch 21/30\n",
      "473/473 [==============================] - 0s 804us/step - loss: 27189.9922 - val_loss: 191186.1719\n",
      "Epoch 22/30\n",
      "473/473 [==============================] - 0s 781us/step - loss: 21299.7363 - val_loss: 230709.7656\n",
      "Epoch 23/30\n",
      "473/473 [==============================] - 0s 762us/step - loss: 20389.6152 - val_loss: 76556.3281\n",
      "Epoch 24/30\n",
      "473/473 [==============================] - 0s 758us/step - loss: 20018.6387 - val_loss: 24232.2852\n",
      "Epoch 25/30\n",
      "473/473 [==============================] - 0s 760us/step - loss: 15829.9463 - val_loss: 378137.7500\n",
      "Epoch 26/30\n",
      "473/473 [==============================] - 0s 752us/step - loss: 14371.1162 - val_loss: 67283.6875\n",
      "Epoch 27/30\n",
      "473/473 [==============================] - 0s 819us/step - loss: 14380.9951 - val_loss: 229447.7188\n",
      "Epoch 28/30\n",
      "473/473 [==============================] - 0s 756us/step - loss: 11408.8164 - val_loss: 147630.8594\n",
      "Epoch 29/30\n",
      "473/473 [==============================] - 0s 776us/step - loss: 10617.6943 - val_loss: 9413.7695\n",
      "Epoch 30/30\n",
      "473/473 [==============================] - 0s 764us/step - loss: 9021.1387 - val_loss: 82894.0859\n",
      "263/263 [==============================] - 0s 503us/step - loss: 17623.0000\n",
      "[CV 2/3] END learning_rate=0.0014732302002701152, n_hidden=2, n_neurons=92, optimizer=adam;, score=-17623.000 total time=  11.9s\n",
      "Epoch 1/30\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 37313584.0000 - val_loss: 4678832.0000\n",
      "Epoch 2/30\n",
      "473/473 [==============================] - 0s 858us/step - loss: 626443.3125 - val_loss: 300088.3125\n",
      "Epoch 3/30\n",
      "473/473 [==============================] - 0s 820us/step - loss: 256229.4219 - val_loss: 141200.9375\n",
      "Epoch 4/30\n",
      "473/473 [==============================] - 0s 822us/step - loss: 99624.3359 - val_loss: 101568.7578\n",
      "Epoch 5/30\n",
      "473/473 [==============================] - 0s 841us/step - loss: 85373.2188 - val_loss: 225967.3906\n",
      "Epoch 6/30\n",
      "473/473 [==============================] - 0s 855us/step - loss: 110635.6797 - val_loss: 128479.1016\n",
      "Epoch 7/30\n",
      "473/473 [==============================] - 0s 809us/step - loss: 68555.3438 - val_loss: 90789.7500\n",
      "Epoch 8/30\n",
      "473/473 [==============================] - 0s 797us/step - loss: 56762.6602 - val_loss: 180240.7344\n",
      "Epoch 9/30\n",
      "473/473 [==============================] - 0s 794us/step - loss: 50520.1406 - val_loss: 43666.2031\n",
      "Epoch 10/30\n",
      "473/473 [==============================] - 0s 801us/step - loss: 21092.6484 - val_loss: 14099.2031\n",
      "Epoch 11/30\n",
      "473/473 [==============================] - 0s 811us/step - loss: 17193.6055 - val_loss: 25927.0039\n",
      "Epoch 12/30\n",
      "473/473 [==============================] - 0s 804us/step - loss: 17074.4414 - val_loss: 43802.9453\n",
      "Epoch 13/30\n",
      "473/473 [==============================] - 0s 955us/step - loss: 15032.7773 - val_loss: 28880.5742\n",
      "Epoch 14/30\n",
      "473/473 [==============================] - 0s 818us/step - loss: 14295.0732 - val_loss: 9695.0137\n",
      "Epoch 15/30\n",
      "473/473 [==============================] - 0s 795us/step - loss: 15568.2236 - val_loss: 18915.5527\n",
      "Epoch 16/30\n",
      "473/473 [==============================] - 0s 826us/step - loss: 12318.3789 - val_loss: 18653.3613\n",
      "Epoch 17/30\n",
      "473/473 [==============================] - 0s 778us/step - loss: 12416.8594 - val_loss: 9048.0566\n",
      "Epoch 18/30\n",
      "473/473 [==============================] - 0s 795us/step - loss: 12249.9619 - val_loss: 99147.2344\n",
      "Epoch 19/30\n",
      "473/473 [==============================] - 0s 772us/step - loss: 12415.4209 - val_loss: 77918.9141\n",
      "Epoch 20/30\n",
      "473/473 [==============================] - 0s 825us/step - loss: 32742.5957 - val_loss: 21448.7773\n",
      "Epoch 21/30\n",
      "473/473 [==============================] - 0s 792us/step - loss: 22874.0918 - val_loss: 28894.4492\n",
      "Epoch 22/30\n",
      "473/473 [==============================] - 0s 795us/step - loss: 13251.0322 - val_loss: 8146.5332\n",
      "Epoch 23/30\n",
      "473/473 [==============================] - 0s 791us/step - loss: 12647.3555 - val_loss: 699.1471\n",
      "Epoch 24/30\n",
      "473/473 [==============================] - 0s 784us/step - loss: 53537.8672 - val_loss: 3496.1562\n",
      "Epoch 25/30\n",
      "473/473 [==============================] - 0s 786us/step - loss: 11379.1387 - val_loss: 2887.5493\n",
      "Epoch 26/30\n",
      "473/473 [==============================] - 0s 828us/step - loss: 9922.2549 - val_loss: 67718.5469\n",
      "Epoch 27/30\n",
      "473/473 [==============================] - 0s 791us/step - loss: 10217.5586 - val_loss: 9569.5928\n",
      "Epoch 28/30\n",
      "473/473 [==============================] - 0s 785us/step - loss: 10814.8496 - val_loss: 7811.6987\n",
      "Epoch 29/30\n",
      "473/473 [==============================] - 0s 788us/step - loss: 9247.1582 - val_loss: 17207.6348\n",
      "Epoch 30/30\n",
      "473/473 [==============================] - 0s 906us/step - loss: 9141.6162 - val_loss: 1653.3389\n",
      "263/263 [==============================] - 0s 510us/step - loss: 588.9484\n",
      "[CV 3/3] END learning_rate=0.0014732302002701152, n_hidden=2, n_neurons=92, optimizer=adam;, score=-588.948 total time=  12.3s\n",
      "Epoch 1/30\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 149484576.0000 - val_loss: 220963.7812\n",
      "Epoch 2/30\n",
      "473/473 [==============================] - 0s 780us/step - loss: 371156.6875 - val_loss: 673497.0625\n",
      "Epoch 3/30\n",
      "473/473 [==============================] - 0s 778us/step - loss: 128033.7188 - val_loss: 422103.7188\n",
      "Epoch 4/30\n",
      "473/473 [==============================] - 0s 796us/step - loss: 161810.9375 - val_loss: 322082.2500\n",
      "Epoch 5/30\n",
      "473/473 [==============================] - 0s 780us/step - loss: 97371.5234 - val_loss: 249761.5000\n",
      "Epoch 6/30\n",
      "473/473 [==============================] - 0s 848us/step - loss: 101260.6641 - val_loss: 252525.0156\n",
      "Epoch 7/30\n",
      "473/473 [==============================] - 0s 782us/step - loss: 70430.5078 - val_loss: 198507.1406\n",
      "Epoch 8/30\n",
      "473/473 [==============================] - 0s 784us/step - loss: 52737.7852 - val_loss: 140187.9688\n",
      "Epoch 9/30\n",
      "473/473 [==============================] - 0s 805us/step - loss: 50041.8047 - val_loss: 158492.5625\n",
      "Epoch 10/30\n",
      "473/473 [==============================] - 0s 805us/step - loss: 53086.8047 - val_loss: 193701.5938\n",
      "Epoch 11/30\n",
      "473/473 [==============================] - 0s 803us/step - loss: 35847.4297 - val_loss: 96474.6406\n",
      "Epoch 12/30\n",
      "473/473 [==============================] - 0s 853us/step - loss: 27475.1230 - val_loss: 92397.8203\n",
      "Epoch 13/30\n",
      "473/473 [==============================] - 0s 822us/step - loss: 23116.8066 - val_loss: 38270.7266\n",
      "Epoch 14/30\n",
      "473/473 [==============================] - 0s 820us/step - loss: 22103.5137 - val_loss: 153351.9531\n",
      "Epoch 15/30\n",
      "473/473 [==============================] - 0s 766us/step - loss: 19707.2695 - val_loss: 62007.7656\n",
      "Epoch 16/30\n",
      "473/473 [==============================] - 0s 880us/step - loss: 18750.0957 - val_loss: 25212.2363\n",
      "Epoch 17/30\n",
      "473/473 [==============================] - 0s 925us/step - loss: 18644.1582 - val_loss: 176057.0469\n",
      "Epoch 18/30\n",
      "473/473 [==============================] - 0s 833us/step - loss: 18533.9570 - val_loss: 153091.5156\n",
      "Epoch 19/30\n",
      "473/473 [==============================] - 0s 819us/step - loss: 19628.1797 - val_loss: 98747.1484\n",
      "Epoch 20/30\n",
      "473/473 [==============================] - 0s 771us/step - loss: 19621.9336 - val_loss: 64954.4062\n",
      "Epoch 21/30\n",
      "473/473 [==============================] - 0s 774us/step - loss: 21051.7617 - val_loss: 408940.6875\n",
      "Epoch 22/30\n",
      "473/473 [==============================] - 0s 795us/step - loss: 23115.6270 - val_loss: 227743.2344\n",
      "Epoch 23/30\n",
      "473/473 [==============================] - 0s 769us/step - loss: 20177.7832 - val_loss: 96139.1562\n",
      "Epoch 24/30\n",
      "473/473 [==============================] - 0s 768us/step - loss: 19000.8496 - val_loss: 146598.9062\n",
      "Epoch 25/30\n",
      "473/473 [==============================] - 0s 760us/step - loss: 23425.1387 - val_loss: 2232.6787\n",
      "Epoch 26/30\n",
      "473/473 [==============================] - 0s 765us/step - loss: 90587.6406 - val_loss: 196550.9062\n",
      "Epoch 27/30\n",
      "473/473 [==============================] - 0s 763us/step - loss: 28815.2578 - val_loss: 271107.9375\n",
      "Epoch 28/30\n",
      "473/473 [==============================] - 0s 806us/step - loss: 26532.6348 - val_loss: 107098.7891\n",
      "Epoch 29/30\n",
      "473/473 [==============================] - 0s 763us/step - loss: 24940.5527 - val_loss: 37137.3516\n",
      "Epoch 30/30\n",
      "473/473 [==============================] - 0s 757us/step - loss: 22460.4004 - val_loss: 302497.9688\n",
      "263/263 [==============================] - 0s 515us/step - loss: 89373.6797\n",
      "[CV 1/3] END learning_rate=0.0003650225911813686, n_hidden=3, n_neurons=53, optimizer=adam;, score=-89373.680 total time=  12.2s\n",
      "Epoch 1/30\n",
      "473/473 [==============================] - 1s 991us/step - loss: 223243616.0000 - val_loss: 38084.6719\n",
      "Epoch 2/30\n",
      "473/473 [==============================] - 0s 900us/step - loss: 400408.2188 - val_loss: 223583.5312\n",
      "Epoch 3/30\n",
      "473/473 [==============================] - 0s 920us/step - loss: 176288.1094 - val_loss: 290091.7812\n",
      "Epoch 4/30\n",
      "473/473 [==============================] - 0s 849us/step - loss: 118140.6797 - val_loss: 241318.5000\n",
      "Epoch 5/30\n",
      "473/473 [==============================] - 0s 830us/step - loss: 79212.8594 - val_loss: 80837.8125\n",
      "Epoch 6/30\n",
      "473/473 [==============================] - 0s 785us/step - loss: 72657.8594 - val_loss: 90840.7812\n",
      "Epoch 7/30\n",
      "473/473 [==============================] - 0s 816us/step - loss: 70399.9609 - val_loss: 89871.2500\n",
      "Epoch 8/30\n",
      "473/473 [==============================] - 0s 760us/step - loss: 72682.7969 - val_loss: 83229.2422\n",
      "Epoch 9/30\n",
      "473/473 [==============================] - 0s 780us/step - loss: 67323.7031 - val_loss: 106741.5156\n",
      "Epoch 10/30\n",
      "473/473 [==============================] - 0s 770us/step - loss: 67475.7656 - val_loss: 89676.1484\n",
      "Epoch 11/30\n",
      "473/473 [==============================] - 0s 775us/step - loss: 62875.9141 - val_loss: 80636.2500\n",
      "263/263 [==============================] - 0s 523us/step - loss: 288009.2812\n",
      "[CV 2/3] END learning_rate=0.0003650225911813686, n_hidden=3, n_neurons=53, optimizer=adam;, score=-288009.281 total time=   5.0s\n",
      "Epoch 1/30\n",
      "473/473 [==============================] - 1s 926us/step - loss: 27842350.0000 - val_loss: 328504.1562\n",
      "Epoch 2/30\n",
      "473/473 [==============================] - 0s 771us/step - loss: 321345.2188 - val_loss: 157997.7656\n",
      "Epoch 3/30\n",
      "473/473 [==============================] - 0s 798us/step - loss: 134060.8438 - val_loss: 156152.6250\n",
      "Epoch 4/30\n",
      "473/473 [==============================] - 0s 767us/step - loss: 85723.2812 - val_loss: 97325.0938\n",
      "Epoch 5/30\n",
      "473/473 [==============================] - 0s 863us/step - loss: 77922.8750 - val_loss: 64683.7695\n",
      "Epoch 6/30\n",
      "473/473 [==============================] - 0s 773us/step - loss: 57659.5859 - val_loss: 54644.4609\n",
      "Epoch 7/30\n",
      "473/473 [==============================] - 0s 792us/step - loss: 62702.6758 - val_loss: 53297.6367\n",
      "Epoch 8/30\n",
      "473/473 [==============================] - 0s 914us/step - loss: 57528.4141 - val_loss: 37612.3594\n",
      "Epoch 9/30\n",
      "473/473 [==============================] - 0s 777us/step - loss: 39486.7773 - val_loss: 70564.5625\n",
      "Epoch 10/30\n",
      "473/473 [==============================] - 0s 857us/step - loss: 34484.6758 - val_loss: 11651.2881\n",
      "Epoch 11/30\n",
      "473/473 [==============================] - 0s 763us/step - loss: 23445.7695 - val_loss: 50074.2578\n",
      "Epoch 12/30\n",
      "473/473 [==============================] - 0s 778us/step - loss: 16307.5420 - val_loss: 77961.8516\n",
      "Epoch 13/30\n",
      "473/473 [==============================] - 0s 788us/step - loss: 14703.7217 - val_loss: 11728.3770\n",
      "Epoch 14/30\n",
      "473/473 [==============================] - 0s 774us/step - loss: 12531.1230 - val_loss: 8041.8179\n",
      "Epoch 15/30\n",
      "473/473 [==============================] - 0s 840us/step - loss: 14318.3203 - val_loss: 65607.3594\n",
      "Epoch 16/30\n",
      "473/473 [==============================] - 0s 763us/step - loss: 15276.9414 - val_loss: 47201.5234\n",
      "Epoch 17/30\n",
      "473/473 [==============================] - 0s 766us/step - loss: 12295.0381 - val_loss: 19267.7012\n",
      "Epoch 18/30\n",
      "473/473 [==============================] - 0s 804us/step - loss: 14959.9395 - val_loss: 7232.6606\n",
      "Epoch 19/30\n",
      "473/473 [==============================] - 0s 793us/step - loss: 18280.6055 - val_loss: 3869.3220\n",
      "Epoch 20/30\n",
      "473/473 [==============================] - 0s 861us/step - loss: 15617.9961 - val_loss: 15184.5947\n",
      "Epoch 21/30\n",
      "473/473 [==============================] - 0s 778us/step - loss: 14935.2666 - val_loss: 14926.8564\n",
      "Epoch 22/30\n",
      "473/473 [==============================] - 0s 771us/step - loss: 13902.0684 - val_loss: 9162.5000\n",
      "Epoch 23/30\n",
      "473/473 [==============================] - 0s 779us/step - loss: 12093.0615 - val_loss: 74984.6953\n",
      "Epoch 24/30\n",
      "473/473 [==============================] - 0s 974us/step - loss: 11483.0264 - val_loss: 31987.5312\n",
      "Epoch 25/30\n",
      "473/473 [==============================] - 0s 794us/step - loss: 10390.6123 - val_loss: 111892.3984\n",
      "Epoch 26/30\n",
      "473/473 [==============================] - 0s 816us/step - loss: 12939.4678 - val_loss: 66440.4609\n",
      "Epoch 27/30\n",
      "473/473 [==============================] - 0s 789us/step - loss: 13657.6641 - val_loss: 2017.2668\n",
      "Epoch 28/30\n",
      "473/473 [==============================] - 0s 784us/step - loss: 9409.1270 - val_loss: 62729.1211\n",
      "Epoch 29/30\n",
      "473/473 [==============================] - 0s 811us/step - loss: 9273.0469 - val_loss: 9625.3516\n",
      "Epoch 30/30\n",
      "473/473 [==============================] - 0s 804us/step - loss: 8265.2080 - val_loss: 38568.0273\n",
      "263/263 [==============================] - 0s 538us/step - loss: 131137.4062\n",
      "[CV 3/3] END learning_rate=0.0003650225911813686, n_hidden=3, n_neurons=53, optimizer=adam;, score=-131137.406 total time=  12.2s\n",
      "Epoch 1/30\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 1006310016.0000 - val_loss: 43999892.0000\n",
      "Epoch 2/30\n",
      "473/473 [==============================] - 0s 867us/step - loss: 6788181.0000 - val_loss: 3661242.2500\n",
      "Epoch 3/30\n",
      "473/473 [==============================] - 0s 866us/step - loss: 1274996.3750 - val_loss: 22569054.0000\n",
      "Epoch 4/30\n",
      "473/473 [==============================] - 0s 835us/step - loss: 481502.7500 - val_loss: 2179166.0000\n",
      "Epoch 5/30\n",
      "473/473 [==============================] - 0s 797us/step - loss: 586862.9375 - val_loss: 2268588.2500\n",
      "Epoch 6/30\n",
      "473/473 [==============================] - 0s 793us/step - loss: 417961.3750 - val_loss: 339585.8438\n",
      "Epoch 7/30\n",
      "473/473 [==============================] - 0s 780us/step - loss: 347801.6875 - val_loss: 53367.5938\n",
      "Epoch 8/30\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 231825.5625 - val_loss: 45856.3398\n",
      "Epoch 9/30\n",
      "473/473 [==============================] - 0s 771us/step - loss: 449595.5312 - val_loss: 2863388.2500\n",
      "Epoch 10/30\n",
      "473/473 [==============================] - 0s 777us/step - loss: 238756.8438 - val_loss: 10327.5449\n",
      "Epoch 11/30\n",
      "473/473 [==============================] - 0s 775us/step - loss: 261089.0625 - val_loss: 78802.4375\n",
      "Epoch 12/30\n",
      "473/473 [==============================] - 0s 794us/step - loss: 378507.3438 - val_loss: 61398.2930\n",
      "Epoch 13/30\n",
      "473/473 [==============================] - 0s 883us/step - loss: 384453.6250 - val_loss: 1154566.1250\n",
      "Epoch 14/30\n",
      "473/473 [==============================] - 0s 777us/step - loss: 276934.2812 - val_loss: 1285946.3750\n",
      "Epoch 15/30\n",
      "473/473 [==============================] - 0s 776us/step - loss: 376758.6875 - val_loss: 6787365.5000\n",
      "Epoch 16/30\n",
      "473/473 [==============================] - 0s 774us/step - loss: 322971.3438 - val_loss: 1367447.5000\n",
      "Epoch 17/30\n",
      "473/473 [==============================] - 0s 793us/step - loss: 348871.9375 - val_loss: 648356.8750\n",
      "Epoch 18/30\n",
      "473/473 [==============================] - 0s 783us/step - loss: 288291.2812 - val_loss: 1117164.3750\n",
      "Epoch 19/30\n",
      "473/473 [==============================] - 0s 864us/step - loss: 368079.5000 - val_loss: 422551.6250\n",
      "Epoch 20/30\n",
      "473/473 [==============================] - 0s 1ms/step - loss: 161516.7188 - val_loss: 413011.7500\n",
      "263/263 [==============================] - 0s 585us/step - loss: 504500.9062\n",
      "[CV 1/3] END learning_rate=0.0011018896957727365, n_hidden=3, n_neurons=70, optimizer=sgd;, score=-504500.906 total time=   8.6s\n",
      "Epoch 1/30\n",
      "473/473 [==============================] - 1s 949us/step - loss: 646784576.0000 - val_loss: 2452102.5000\n",
      "Epoch 2/30\n",
      "473/473 [==============================] - 0s 780us/step - loss: 15395204.0000 - val_loss: 1380638.8750\n",
      "Epoch 3/30\n",
      "473/473 [==============================] - 0s 803us/step - loss: 921424.7500 - val_loss: 776526.7500\n",
      "Epoch 4/30\n",
      "473/473 [==============================] - 0s 776us/step - loss: 267258.4688 - val_loss: 1962283.2500\n",
      "Epoch 5/30\n",
      "473/473 [==============================] - 0s 767us/step - loss: 442143.4062 - val_loss: 7748197.5000\n",
      "Epoch 6/30\n",
      "473/473 [==============================] - 0s 781us/step - loss: 327867.5625 - val_loss: 1377668.0000\n",
      "Epoch 7/30\n",
      "473/473 [==============================] - 0s 805us/step - loss: 344097.8438 - val_loss: 2141858.0000\n",
      "Epoch 8/30\n",
      "473/473 [==============================] - 0s 806us/step - loss: 236323.5156 - val_loss: 138364.0469\n",
      "Epoch 9/30\n",
      "473/473 [==============================] - 0s 793us/step - loss: 231829.5000 - val_loss: 2381198.0000\n",
      "Epoch 10/30\n",
      "473/473 [==============================] - 0s 841us/step - loss: 319547.3125 - val_loss: 390463.1562\n",
      "Epoch 11/30\n",
      "473/473 [==============================] - 0s 850us/step - loss: 275208.9375 - val_loss: 5321217.0000\n",
      "Epoch 12/30\n",
      "473/473 [==============================] - 0s 771us/step - loss: 183967.4375 - val_loss: 881705.1250\n",
      "Epoch 13/30\n",
      "473/473 [==============================] - 0s 763us/step - loss: 209055.4375 - val_loss: 5337851.5000\n",
      "Epoch 14/30\n",
      "473/473 [==============================] - 0s 891us/step - loss: 235825.3438 - val_loss: 170970.8281\n",
      "Epoch 15/30\n",
      "473/473 [==============================] - 0s 843us/step - loss: 166781.2969 - val_loss: 3621411.2500\n",
      "Epoch 16/30\n",
      "473/473 [==============================] - 0s 849us/step - loss: 155663.5781 - val_loss: 1462687.5000\n",
      "Epoch 17/30\n",
      "473/473 [==============================] - 0s 807us/step - loss: 270455.0938 - val_loss: 199456.6094\n",
      "Epoch 18/30\n",
      "473/473 [==============================] - 0s 776us/step - loss: 195770.3125 - val_loss: 779820.1250\n",
      "263/263 [==============================] - 0s 508us/step - loss: 96647.3359\n",
      "[CV 2/3] END learning_rate=0.0011018896957727365, n_hidden=3, n_neurons=70, optimizer=sgd;, score=-96647.336 total time=   7.6s\n",
      "Epoch 1/30\n",
      "473/473 [==============================] - 1s 943us/step - loss: 199054288.0000 - val_loss: 69950.2188\n",
      "Epoch 2/30\n",
      "473/473 [==============================] - 0s 754us/step - loss: 4633214.0000 - val_loss: 15804.8818\n",
      "Epoch 3/30\n",
      "473/473 [==============================] - 0s 851us/step - loss: 501277.5625 - val_loss: 101628.1719\n",
      "Epoch 4/30\n",
      "473/473 [==============================] - 0s 810us/step - loss: 351254.1250 - val_loss: 5305272.5000\n",
      "Epoch 5/30\n",
      "473/473 [==============================] - 0s 762us/step - loss: 460189.5312 - val_loss: 471100.9062\n",
      "Epoch 6/30\n",
      "473/473 [==============================] - 0s 748us/step - loss: 476224.7500 - val_loss: 1643218.6250\n",
      "Epoch 7/30\n",
      "473/473 [==============================] - 0s 761us/step - loss: 486688.8750 - val_loss: 2582951.5000\n",
      "Epoch 8/30\n",
      "473/473 [==============================] - 0s 761us/step - loss: 429029.9062 - val_loss: 1289281.3750\n",
      "Epoch 9/30\n",
      "473/473 [==============================] - 0s 766us/step - loss: 228671.5000 - val_loss: 186511.3906\n",
      "Epoch 10/30\n",
      "473/473 [==============================] - 0s 750us/step - loss: 210193.0625 - val_loss: 68483.6094\n",
      "Epoch 11/30\n",
      "473/473 [==============================] - 0s 870us/step - loss: 408475.7500 - val_loss: 191604.0000\n",
      "Epoch 12/30\n",
      "473/473 [==============================] - 0s 761us/step - loss: 159731.9531 - val_loss: 118889.7188\n",
      "263/263 [==============================] - 0s 503us/step - loss: 251771.8750\n",
      "[CV 3/3] END learning_rate=0.0011018896957727365, n_hidden=3, n_neurons=70, optimizer=sgd;, score=-251771.875 total time=   5.1s\n",
      "Epoch 1/30\n",
      "473/473 [==============================] - 1s 893us/step - loss: 813184.4375 - val_loss: 164633.0625\n",
      "Epoch 2/30\n",
      "473/473 [==============================] - 0s 702us/step - loss: 179897.7031 - val_loss: 161666.7812\n",
      "Epoch 3/30\n",
      "473/473 [==============================] - 0s 704us/step - loss: 170717.0625 - val_loss: 158885.5938\n",
      "Epoch 4/30\n",
      "473/473 [==============================] - 0s 687us/step - loss: 175120.9688 - val_loss: 156074.4844\n",
      "Epoch 5/30\n",
      "473/473 [==============================] - 0s 698us/step - loss: 167655.4219 - val_loss: 153386.2656\n",
      "Epoch 6/30\n",
      "473/473 [==============================] - 0s 698us/step - loss: 162057.5469 - val_loss: 150556.1406\n",
      "Epoch 7/30\n",
      "473/473 [==============================] - 0s 698us/step - loss: 161899.1562 - val_loss: 147561.0625\n",
      "Epoch 8/30\n",
      "473/473 [==============================] - 0s 713us/step - loss: 152043.2031 - val_loss: 143874.0000\n",
      "Epoch 9/30\n",
      "473/473 [==============================] - 0s 696us/step - loss: 147120.9531 - val_loss: 139187.7812\n",
      "Epoch 10/30\n",
      "473/473 [==============================] - 0s 698us/step - loss: 141684.0625 - val_loss: 132975.5156\n",
      "Epoch 11/30\n",
      "473/473 [==============================] - 0s 713us/step - loss: 135572.7188 - val_loss: 124542.2031\n",
      "Epoch 12/30\n",
      "473/473 [==============================] - 0s 700us/step - loss: 130915.9531 - val_loss: 113840.2578\n",
      "Epoch 13/30\n",
      "473/473 [==============================] - 0s 696us/step - loss: 120993.5078 - val_loss: 97816.1953\n",
      "Epoch 14/30\n",
      "473/473 [==============================] - 0s 700us/step - loss: 125043.6094 - val_loss: 74363.8047\n",
      "Epoch 15/30\n",
      "473/473 [==============================] - 0s 691us/step - loss: 106589.3438 - val_loss: 43781.3008\n",
      "Epoch 16/30\n",
      "473/473 [==============================] - 0s 705us/step - loss: 83915.5234 - val_loss: 13914.4248\n",
      "Epoch 17/30\n",
      "473/473 [==============================] - 0s 796us/step - loss: 65075.1289 - val_loss: 497.5622\n",
      "Epoch 18/30\n",
      "473/473 [==============================] - 0s 696us/step - loss: 57376.1094 - val_loss: 1734.4255\n",
      "Epoch 19/30\n",
      "473/473 [==============================] - 0s 713us/step - loss: 53227.4023 - val_loss: 4387.5669\n",
      "Epoch 20/30\n",
      "473/473 [==============================] - 0s 696us/step - loss: 54186.1211 - val_loss: 5147.2178\n",
      "Epoch 21/30\n",
      "473/473 [==============================] - 0s 694us/step - loss: 54507.8203 - val_loss: 5237.8618\n",
      "Epoch 22/30\n",
      "473/473 [==============================] - 0s 700us/step - loss: 54040.4883 - val_loss: 5130.5454\n",
      "Epoch 23/30\n",
      "473/473 [==============================] - 0s 691us/step - loss: 52633.4844 - val_loss: 5403.5410\n",
      "Epoch 24/30\n",
      "473/473 [==============================] - 0s 696us/step - loss: 52753.3047 - val_loss: 5325.2075\n",
      "Epoch 25/30\n",
      "473/473 [==============================] - 0s 683us/step - loss: 51792.0781 - val_loss: 5329.8940\n",
      "Epoch 26/30\n",
      "473/473 [==============================] - 0s 696us/step - loss: 50879.6875 - val_loss: 5543.5195\n",
      "Epoch 27/30\n",
      "473/473 [==============================] - 0s 704us/step - loss: 50757.4805 - val_loss: 5456.5508\n",
      "263/263 [==============================] - 0s 508us/step - loss: 85464.3906\n",
      "[CV 1/3] END learning_rate=0.0009677735703570196, n_hidden=4, n_neurons=6, optimizer=adam;, score=-85464.391 total time=   9.9s\n",
      "Epoch 1/30\n",
      "473/473 [==============================] - 1s 911us/step - loss: 97535.6875 - val_loss: 83116.9609\n",
      "Epoch 2/30\n",
      "473/473 [==============================] - 0s 716us/step - loss: 74120.4531 - val_loss: 62120.4336\n",
      "Epoch 3/30\n",
      "473/473 [==============================] - 0s 752us/step - loss: 63447.6016 - val_loss: 84887.5000\n",
      "Epoch 4/30\n",
      "473/473 [==============================] - 0s 798us/step - loss: 49540.6914 - val_loss: 76034.1562\n",
      "Epoch 5/30\n",
      "473/473 [==============================] - 0s 870us/step - loss: 36071.1133 - val_loss: 74961.2188\n",
      "Epoch 6/30\n",
      "473/473 [==============================] - 0s 737us/step - loss: 22103.6348 - val_loss: 1813.5898\n",
      "Epoch 7/30\n",
      "473/473 [==============================] - 0s 726us/step - loss: 20839.9199 - val_loss: 131166.4375\n",
      "Epoch 8/30\n",
      "473/473 [==============================] - 0s 742us/step - loss: 17781.1504 - val_loss: 161614.6406\n",
      "Epoch 9/30\n",
      "473/473 [==============================] - 0s 815us/step - loss: 17588.0176 - val_loss: 3538.8477\n",
      "Epoch 10/30\n",
      "473/473 [==============================] - 0s 822us/step - loss: 16359.2852 - val_loss: 14889.1650\n",
      "Epoch 11/30\n",
      "473/473 [==============================] - 0s 816us/step - loss: 16075.8926 - val_loss: 39010.5195\n",
      "Epoch 12/30\n",
      "473/473 [==============================] - 0s 748us/step - loss: 15625.7119 - val_loss: 59638.0195\n",
      "Epoch 13/30\n",
      "473/473 [==============================] - 0s 723us/step - loss: 15330.5195 - val_loss: 122179.1641\n",
      "Epoch 14/30\n",
      "473/473 [==============================] - 0s 723us/step - loss: 15486.1982 - val_loss: 33025.9648\n",
      "Epoch 15/30\n",
      "473/473 [==============================] - 0s 751us/step - loss: 14925.4043 - val_loss: 83524.3438\n",
      "Epoch 16/30\n",
      "473/473 [==============================] - 0s 731us/step - loss: 14756.1621 - val_loss: 6470.2046\n",
      "263/263 [==============================] - 0s 501us/step - loss: 11923.4355\n",
      "[CV 2/3] END learning_rate=0.0009677735703570196, n_hidden=4, n_neurons=6, optimizer=adam;, score=-11923.436 total time=   6.6s\n",
      "Epoch 1/30\n",
      "473/473 [==============================] - 1s 900us/step - loss: 1562326528.0000 - val_loss: 3158383.7500\n",
      "Epoch 2/30\n",
      "473/473 [==============================] - 0s 826us/step - loss: 5654516.5000 - val_loss: 98641.0859\n",
      "Epoch 3/30\n",
      "473/473 [==============================] - 0s 721us/step - loss: 951358.8125 - val_loss: 61943.6836\n",
      "Epoch 4/30\n",
      "473/473 [==============================] - 0s 715us/step - loss: 176274.3750 - val_loss: 66098.1953\n",
      "Epoch 5/30\n",
      "473/473 [==============================] - 0s 723us/step - loss: 75607.6641 - val_loss: 45694.1523\n",
      "Epoch 6/30\n",
      "473/473 [==============================] - 0s 723us/step - loss: 71907.9688 - val_loss: 53831.6172\n",
      "Epoch 7/30\n",
      "473/473 [==============================] - 0s 721us/step - loss: 71286.7344 - val_loss: 37105.2383\n",
      "Epoch 8/30\n",
      "473/473 [==============================] - 0s 719us/step - loss: 70983.5234 - val_loss: 55472.0508\n",
      "Epoch 9/30\n",
      "473/473 [==============================] - 0s 739us/step - loss: 72329.9219 - val_loss: 45188.2305\n",
      "Epoch 10/30\n",
      "473/473 [==============================] - 0s 728us/step - loss: 70679.6250 - val_loss: 45001.4688\n",
      "Epoch 11/30\n",
      "473/473 [==============================] - 0s 751us/step - loss: 70883.1641 - val_loss: 41154.9805\n",
      "Epoch 12/30\n",
      "473/473 [==============================] - 0s 808us/step - loss: 70721.1641 - val_loss: 52754.6523\n",
      "Epoch 13/30\n",
      "473/473 [==============================] - 0s 740us/step - loss: 70182.9453 - val_loss: 58883.1719\n",
      "Epoch 14/30\n",
      "473/473 [==============================] - 0s 737us/step - loss: 69288.7734 - val_loss: 43247.9141\n",
      "Epoch 15/30\n",
      "473/473 [==============================] - 0s 857us/step - loss: 70567.6328 - val_loss: 79114.2109\n",
      "Epoch 16/30\n",
      "473/473 [==============================] - 0s 753us/step - loss: 69587.8828 - val_loss: 44146.2617\n",
      "Epoch 17/30\n",
      "473/473 [==============================] - 0s 739us/step - loss: 68835.9688 - val_loss: 63546.9062\n",
      "263/263 [==============================] - 0s 519us/step - loss: 566041.5625\n",
      "[CV 3/3] END learning_rate=0.0009677735703570196, n_hidden=4, n_neurons=6, optimizer=adam;, score=-566041.562 total time=   6.9s\n",
      "Epoch 1/30\n",
      "473/473 [==============================] - 1s 952us/step - loss: 469955328.0000 - val_loss: 9657787.0000\n",
      "Epoch 2/30\n",
      "473/473 [==============================] - 0s 756us/step - loss: 59330800.0000 - val_loss: 13399504.0000\n",
      "Epoch 3/30\n",
      "473/473 [==============================] - 0s 816us/step - loss: 9219937.0000 - val_loss: 7048033.0000\n",
      "Epoch 4/30\n",
      "473/473 [==============================] - 0s 757us/step - loss: 2251114.0000 - val_loss: 3607698.0000\n",
      "Epoch 5/30\n",
      "473/473 [==============================] - 0s 752us/step - loss: 927311.8125 - val_loss: 2577413.5000\n",
      "Epoch 6/30\n",
      "473/473 [==============================] - 0s 754us/step - loss: 491216.6562 - val_loss: 920551.1250\n",
      "Epoch 7/30\n",
      "473/473 [==============================] - 0s 815us/step - loss: 293563.0000 - val_loss: 625479.3750\n",
      "Epoch 8/30\n",
      "473/473 [==============================] - 0s 750us/step - loss: 228817.9688 - val_loss: 677447.1250\n",
      "Epoch 9/30\n",
      "473/473 [==============================] - 0s 757us/step - loss: 139431.3750 - val_loss: 133009.5312\n",
      "Epoch 10/30\n",
      "473/473 [==============================] - 0s 909us/step - loss: 161452.5938 - val_loss: 160605.4688\n",
      "Epoch 11/30\n",
      "473/473 [==============================] - 0s 801us/step - loss: 179556.4375 - val_loss: 196655.2812\n",
      "Epoch 12/30\n",
      "473/473 [==============================] - 0s 778us/step - loss: 151362.2969 - val_loss: 923979.8750\n",
      "Epoch 13/30\n",
      "473/473 [==============================] - 0s 778us/step - loss: 118067.8516 - val_loss: 301138.0938\n",
      "Epoch 14/30\n",
      "473/473 [==============================] - 0s 826us/step - loss: 81154.8906 - val_loss: 71510.0625\n",
      "Epoch 15/30\n",
      "473/473 [==============================] - 0s 778us/step - loss: 70423.2188 - val_loss: 1020941.5625\n",
      "Epoch 16/30\n",
      "473/473 [==============================] - 0s 773us/step - loss: 158658.3594 - val_loss: 93498.1250\n",
      "Epoch 17/30\n",
      "473/473 [==============================] - 0s 806us/step - loss: 100184.6406 - val_loss: 137228.9844\n",
      "Epoch 18/30\n",
      "473/473 [==============================] - 0s 835us/step - loss: 107497.2891 - val_loss: 144910.2969\n",
      "Epoch 19/30\n",
      "473/473 [==============================] - 0s 790us/step - loss: 75877.8047 - val_loss: 814819.5625\n",
      "Epoch 20/30\n",
      "473/473 [==============================] - 0s 808us/step - loss: 92811.6641 - val_loss: 2008218.7500\n",
      "Epoch 21/30\n",
      "473/473 [==============================] - 0s 804us/step - loss: 116802.9531 - val_loss: 912385.0000\n",
      "Epoch 22/30\n",
      "473/473 [==============================] - 0s 919us/step - loss: 123685.5391 - val_loss: 644731.0625\n",
      "Epoch 23/30\n",
      "473/473 [==============================] - 0s 811us/step - loss: 130197.1328 - val_loss: 192038.5625\n",
      "Epoch 24/30\n",
      "473/473 [==============================] - 0s 779us/step - loss: 84911.4297 - val_loss: 319601.8125\n",
      "263/263 [==============================] - 0s 524us/step - loss: 342668.9688\n",
      "[CV 1/3] END learning_rate=0.0005518963375383896, n_hidden=3, n_neurons=52, optimizer=sgd;, score=-342668.969 total time=   9.8s\n",
      "Epoch 1/30\n",
      "473/473 [==============================] - 1s 952us/step - loss: 699943552.0000 - val_loss: 5131184.0000\n",
      "Epoch 2/30\n",
      "473/473 [==============================] - 0s 772us/step - loss: 99262992.0000 - val_loss: 2017928.2500\n",
      "Epoch 3/30\n",
      "473/473 [==============================] - 0s 794us/step - loss: 17436756.0000 - val_loss: 1739591.7500\n",
      "Epoch 4/30\n",
      "473/473 [==============================] - 0s 797us/step - loss: 5401373.5000 - val_loss: 784958.9375\n",
      "Epoch 5/30\n",
      "473/473 [==============================] - 0s 841us/step - loss: 1759794.6250 - val_loss: 419563.7188\n",
      "Epoch 6/30\n",
      "473/473 [==============================] - 0s 779us/step - loss: 905490.3125 - val_loss: 109755.7500\n",
      "Epoch 7/30\n",
      "473/473 [==============================] - 0s 772us/step - loss: 569555.8125 - val_loss: 61638.0938\n",
      "Epoch 8/30\n",
      "473/473 [==============================] - 0s 1ms/step - loss: 475283.8750 - val_loss: 1187103.2500\n",
      "Epoch 9/30\n",
      "473/473 [==============================] - 0s 765us/step - loss: 286886.0312 - val_loss: 929993.2500\n",
      "Epoch 10/30\n",
      "473/473 [==============================] - 0s 769us/step - loss: 375226.9688 - val_loss: 446599.8750\n",
      "Epoch 11/30\n",
      "473/473 [==============================] - 0s 766us/step - loss: 239248.4375 - val_loss: 51315.3281\n",
      "Epoch 12/30\n",
      "473/473 [==============================] - 0s 873us/step - loss: 260848.8906 - val_loss: 2926877.2500\n",
      "Epoch 13/30\n",
      "473/473 [==============================] - 0s 778us/step - loss: 237658.5625 - val_loss: 1041310.1250\n",
      "Epoch 14/30\n",
      "473/473 [==============================] - 0s 790us/step - loss: 267061.8125 - val_loss: 529173.5000\n",
      "Epoch 15/30\n",
      "473/473 [==============================] - 0s 826us/step - loss: 257087.5469 - val_loss: 1609424.8750\n",
      "Epoch 16/30\n",
      "473/473 [==============================] - 0s 795us/step - loss: 290606.3438 - val_loss: 1099711.2500\n",
      "Epoch 17/30\n",
      "473/473 [==============================] - 0s 760us/step - loss: 206143.0000 - val_loss: 258232.9062\n",
      "Epoch 18/30\n",
      "473/473 [==============================] - 0s 763us/step - loss: 245690.8906 - val_loss: 2638185.2500\n",
      "Epoch 19/30\n",
      "473/473 [==============================] - 0s 807us/step - loss: 182260.7031 - val_loss: 289166.3438\n",
      "Epoch 20/30\n",
      "473/473 [==============================] - 0s 790us/step - loss: 234817.0781 - val_loss: 2325472.5000\n",
      "Epoch 21/30\n",
      "473/473 [==============================] - 0s 853us/step - loss: 133434.9219 - val_loss: 87104.2891\n",
      "263/263 [==============================] - 0s 527us/step - loss: 283817.8125\n",
      "[CV 2/3] END learning_rate=0.0005518963375383896, n_hidden=3, n_neurons=52, optimizer=sgd;, score=-283817.812 total time=   8.8s\n",
      "Epoch 1/30\n",
      "473/473 [==============================] - 1s 981us/step - loss: 340177280.0000 - val_loss: 3109709.5000\n",
      "Epoch 2/30\n",
      "473/473 [==============================] - 0s 759us/step - loss: 72821528.0000 - val_loss: 6103298.0000\n",
      "Epoch 3/30\n",
      "473/473 [==============================] - 0s 759us/step - loss: 20653848.0000 - val_loss: 1750810.7500\n",
      "Epoch 4/30\n",
      "473/473 [==============================] - 0s 820us/step - loss: 5666381.0000 - val_loss: 426365.5312\n",
      "Epoch 5/30\n",
      "473/473 [==============================] - 0s 759us/step - loss: 1433236.2500 - val_loss: 267783.0625\n",
      "Epoch 6/30\n",
      "473/473 [==============================] - 0s 761us/step - loss: 433913.5625 - val_loss: 218789.4844\n",
      "Epoch 7/30\n",
      "473/473 [==============================] - 0s 762us/step - loss: 363081.3438 - val_loss: 115666.9141\n",
      "Epoch 8/30\n",
      "473/473 [==============================] - 0s 802us/step - loss: 144523.7188 - val_loss: 646050.4375\n",
      "Epoch 9/30\n",
      "473/473 [==============================] - 0s 756us/step - loss: 168463.0000 - val_loss: 529111.5625\n",
      "Epoch 10/30\n",
      "473/473 [==============================] - 0s 755us/step - loss: 113661.6172 - val_loss: 255835.3125\n",
      "Epoch 11/30\n",
      "473/473 [==============================] - 0s 813us/step - loss: 116786.3359 - val_loss: 333667.6875\n",
      "Epoch 12/30\n",
      "473/473 [==============================] - 0s 952us/step - loss: 126962.9453 - val_loss: 441092.2812\n",
      "Epoch 13/30\n",
      "473/473 [==============================] - 0s 765us/step - loss: 181956.6719 - val_loss: 53451.6797\n",
      "Epoch 14/30\n",
      "473/473 [==============================] - 0s 745us/step - loss: 123830.6719 - val_loss: 127677.4688\n",
      "Epoch 15/30\n",
      "473/473 [==============================] - 0s 785us/step - loss: 108182.4688 - val_loss: 75401.3594\n",
      "Epoch 16/30\n",
      "473/473 [==============================] - 0s 739us/step - loss: 104714.8281 - val_loss: 340443.1875\n",
      "Epoch 17/30\n",
      "473/473 [==============================] - 0s 742us/step - loss: 101288.6250 - val_loss: 71932.0156\n",
      "Epoch 18/30\n",
      "473/473 [==============================] - 0s 800us/step - loss: 118240.2266 - val_loss: 64216.4180\n",
      "Epoch 19/30\n",
      "473/473 [==============================] - 0s 752us/step - loss: 80034.2812 - val_loss: 482901.0000\n",
      "Epoch 20/30\n",
      "473/473 [==============================] - 0s 752us/step - loss: 91401.3281 - val_loss: 90146.9297\n",
      "Epoch 21/30\n",
      "473/473 [==============================] - 0s 869us/step - loss: 85660.8594 - val_loss: 631733.3750\n",
      "Epoch 22/30\n",
      "473/473 [==============================] - 0s 862us/step - loss: 82289.4375 - val_loss: 254639.6094\n",
      "Epoch 23/30\n",
      "473/473 [==============================] - 0s 867us/step - loss: 82310.4609 - val_loss: 144132.4062\n",
      "263/263 [==============================] - 0s 523us/step - loss: 456284.4688\n",
      "[CV 3/3] END learning_rate=0.0005518963375383896, n_hidden=3, n_neurons=52, optimizer=sgd;, score=-456284.469 total time=   9.3s\n",
      "Epoch 1/30\n",
      "473/473 [==============================] - 1s 942us/step - loss: 647228096.0000 - val_loss: 27637490.0000\n",
      "Epoch 2/30\n",
      "473/473 [==============================] - 0s 736us/step - loss: 6040306.0000 - val_loss: 1311186.3750\n",
      "Epoch 3/30\n",
      "473/473 [==============================] - 0s 721us/step - loss: 1381267.5000 - val_loss: 633857.1875\n",
      "Epoch 4/30\n",
      "473/473 [==============================] - 0s 714us/step - loss: 389629.2500 - val_loss: 1202567.8750\n",
      "Epoch 5/30\n",
      "473/473 [==============================] - 0s 757us/step - loss: 335205.8438 - val_loss: 2971008.5000\n",
      "Epoch 6/30\n",
      "473/473 [==============================] - 0s 735us/step - loss: 545517.5000 - val_loss: 4080114.0000\n",
      "Epoch 7/30\n",
      "473/473 [==============================] - 0s 725us/step - loss: 170953.9219 - val_loss: 1359036.0000\n",
      "Epoch 8/30\n",
      "473/473 [==============================] - 0s 770us/step - loss: 136148.8438 - val_loss: 588713.5625\n",
      "Epoch 9/30\n",
      "473/473 [==============================] - 0s 739us/step - loss: 447315.5938 - val_loss: 298243.3750\n",
      "Epoch 10/30\n",
      "473/473 [==============================] - 0s 734us/step - loss: 113807.0625 - val_loss: 738772.7500\n",
      "Epoch 11/30\n",
      "473/473 [==============================] - 0s 763us/step - loss: 213846.6875 - val_loss: 884936.2500\n",
      "Epoch 12/30\n",
      "473/473 [==============================] - 0s 1ms/step - loss: 203839.3594 - val_loss: 208149.0156\n",
      "Epoch 13/30\n",
      "473/473 [==============================] - 0s 814us/step - loss: 209904.7812 - val_loss: 553768.6875\n",
      "Epoch 14/30\n",
      "473/473 [==============================] - 0s 768us/step - loss: 474129.5625 - val_loss: 2379947.7500\n",
      "Epoch 15/30\n",
      "473/473 [==============================] - 0s 809us/step - loss: 129360.0547 - val_loss: 732186.7500\n",
      "Epoch 16/30\n",
      "473/473 [==============================] - 0s 723us/step - loss: 95725.8672 - val_loss: 3242664.0000\n",
      "Epoch 17/30\n",
      "473/473 [==============================] - 0s 725us/step - loss: 403098.9375 - val_loss: 163209.6719\n",
      "Epoch 18/30\n",
      "473/473 [==============================] - 0s 728us/step - loss: 117537.3516 - val_loss: 1602632.7500\n",
      "Epoch 19/30\n",
      "473/473 [==============================] - 0s 766us/step - loss: 169811.1250 - val_loss: 514703.1875\n",
      "Epoch 20/30\n",
      "473/473 [==============================] - 0s 735us/step - loss: 523760.9375 - val_loss: 159470.9531\n",
      "Epoch 21/30\n",
      "473/473 [==============================] - 0s 733us/step - loss: 147338.4531 - val_loss: 280655.0625\n",
      "Epoch 22/30\n",
      "473/473 [==============================] - 0s 783us/step - loss: 142485.1250 - val_loss: 1068472.6250\n",
      "Epoch 23/30\n",
      "473/473 [==============================] - 0s 733us/step - loss: 264241.0000 - val_loss: 809983.3125\n",
      "Epoch 24/30\n",
      "473/473 [==============================] - 0s 735us/step - loss: 103432.6172 - val_loss: 751713.5625\n",
      "Epoch 25/30\n",
      "473/473 [==============================] - 0s 925us/step - loss: 89726.0703 - val_loss: 214163.7656\n",
      "Epoch 26/30\n",
      "473/473 [==============================] - 0s 744us/step - loss: 152082.5938 - val_loss: 1395851.6250\n",
      "Epoch 27/30\n",
      "473/473 [==============================] - 0s 737us/step - loss: 103829.6094 - val_loss: 405271.4375\n",
      "Epoch 28/30\n",
      "473/473 [==============================] - 0s 736us/step - loss: 80306.7578 - val_loss: 66573.5391\n",
      "Epoch 29/30\n",
      "473/473 [==============================] - 0s 808us/step - loss: 178854.9062 - val_loss: 1172815.0000\n",
      "Epoch 30/30\n",
      "473/473 [==============================] - 0s 771us/step - loss: 108217.8906 - val_loss: 498408.0625\n",
      "263/263 [==============================] - 0s 519us/step - loss: 413601.4375\n",
      "[CV 1/3] END learning_rate=0.0029282007084415595, n_hidden=2, n_neurons=55, optimizer=sgd;, score=-413601.438 total time=  11.5s\n",
      "Epoch 1/30\n",
      "473/473 [==============================] - 1s 943us/step - loss: 2492337664.0000 - val_loss: 2249418.0000\n",
      "Epoch 2/30\n",
      "473/473 [==============================] - 0s 731us/step - loss: 3173181.7500 - val_loss: 3817861.0000\n",
      "Epoch 3/30\n",
      "473/473 [==============================] - 0s 706us/step - loss: 376467.1562 - val_loss: 8051048.5000\n",
      "Epoch 4/30\n",
      "473/473 [==============================] - 0s 708us/step - loss: 322415.9375 - val_loss: 150901.3906\n",
      "Epoch 5/30\n",
      "473/473 [==============================] - 0s 763us/step - loss: 189037.4219 - val_loss: 1447771.3750\n",
      "Epoch 6/30\n",
      "473/473 [==============================] - 0s 711us/step - loss: 167855.4531 - val_loss: 2252487.7500\n",
      "Epoch 7/30\n",
      "473/473 [==============================] - 0s 859us/step - loss: 169261.5469 - val_loss: 3545921.5000\n",
      "Epoch 8/30\n",
      "473/473 [==============================] - 0s 786us/step - loss: 142121.9531 - val_loss: 1082967.6250\n",
      "Epoch 9/30\n",
      "473/473 [==============================] - 0s 743us/step - loss: 123686.1484 - val_loss: 578071.3750\n",
      "Epoch 10/30\n",
      "473/473 [==============================] - 0s 741us/step - loss: 151648.5156 - val_loss: 792467.1875\n",
      "Epoch 11/30\n",
      "473/473 [==============================] - 0s 740us/step - loss: 138299.1875 - val_loss: 404187.8438\n",
      "Epoch 12/30\n",
      "473/473 [==============================] - 0s 772us/step - loss: 105275.0938 - val_loss: 7047460.0000\n",
      "Epoch 13/30\n",
      "473/473 [==============================] - 0s 741us/step - loss: 151049.3750 - val_loss: 300431.2188\n",
      "Epoch 14/30\n",
      "473/473 [==============================] - 0s 719us/step - loss: 120858.9609 - val_loss: 75961.0078\n",
      "Epoch 15/30\n",
      "473/473 [==============================] - 0s 772us/step - loss: 181776.5312 - val_loss: 2054758.6250\n",
      "Epoch 16/30\n",
      "473/473 [==============================] - 0s 729us/step - loss: 100835.6641 - val_loss: 209169.5312\n",
      "Epoch 17/30\n",
      "473/473 [==============================] - 0s 729us/step - loss: 107372.5156 - val_loss: 335222.2188\n",
      "Epoch 18/30\n",
      "473/473 [==============================] - 0s 765us/step - loss: 176614.0625 - val_loss: 265230.0625\n",
      "Epoch 19/30\n",
      "473/473 [==============================] - 0s 840us/step - loss: 150935.3750 - val_loss: 5027517.0000\n",
      "Epoch 20/30\n",
      "473/473 [==============================] - 0s 729us/step - loss: 164471.5469 - val_loss: 798459.6875\n",
      "Epoch 21/30\n",
      "473/473 [==============================] - 0s 728us/step - loss: 149720.1406 - val_loss: 3054328.0000\n",
      "Epoch 22/30\n",
      "473/473 [==============================] - 0s 783us/step - loss: 136243.8594 - val_loss: 46846.8398\n",
      "Epoch 23/30\n",
      "473/473 [==============================] - 0s 722us/step - loss: 90067.1641 - val_loss: 720803.5000\n",
      "Epoch 24/30\n",
      "473/473 [==============================] - 0s 718us/step - loss: 71237.7109 - val_loss: 233426.7656\n",
      "Epoch 25/30\n",
      "473/473 [==============================] - 0s 753us/step - loss: 64045.3516 - val_loss: 23300.8945\n",
      "Epoch 26/30\n",
      "473/473 [==============================] - 0s 719us/step - loss: 92182.6484 - val_loss: 853733.3750\n",
      "Epoch 27/30\n",
      "473/473 [==============================] - 0s 710us/step - loss: 65608.0312 - val_loss: 114062.1719\n",
      "Epoch 28/30\n",
      "473/473 [==============================] - 0s 718us/step - loss: 76791.3125 - val_loss: 223585.2812\n",
      "Epoch 29/30\n",
      "473/473 [==============================] - 0s 770us/step - loss: 73182.6875 - val_loss: 13965.3213\n",
      "Epoch 30/30\n",
      "473/473 [==============================] - 0s 714us/step - loss: 70171.8750 - val_loss: 81085.5781\n",
      "263/263 [==============================] - 0s 504us/step - loss: 68663.4141\n",
      "[CV 2/3] END learning_rate=0.0029282007084415595, n_hidden=2, n_neurons=55, optimizer=sgd;, score=-68663.414 total time=  11.2s\n",
      "Epoch 1/30\n",
      "473/473 [==============================] - 1s 929us/step - loss: 53865360.0000 - val_loss: 343368.8125\n",
      "Epoch 2/30\n",
      "473/473 [==============================] - 0s 701us/step - loss: 2734755.2500 - val_loss: 1344225.8750\n",
      "Epoch 3/30\n",
      "473/473 [==============================] - 0s 699us/step - loss: 689484.6250 - val_loss: 1553773.6250\n",
      "Epoch 4/30\n",
      "473/473 [==============================] - 0s 697us/step - loss: 685827.0625 - val_loss: 1517600.6250\n",
      "Epoch 5/30\n",
      "473/473 [==============================] - 0s 753us/step - loss: 377123.7188 - val_loss: 128762.9219\n",
      "Epoch 6/30\n",
      "473/473 [==============================] - 0s 702us/step - loss: 262925.6250 - val_loss: 1138124.5000\n",
      "Epoch 7/30\n",
      "473/473 [==============================] - 0s 709us/step - loss: 276780.6250 - val_loss: 77168.9219\n",
      "Epoch 8/30\n",
      "473/473 [==============================] - 0s 750us/step - loss: 250286.3594 - val_loss: 136921.8750\n",
      "Epoch 9/30\n",
      "473/473 [==============================] - 0s 701us/step - loss: 236785.1406 - val_loss: 460616.0312\n",
      "Epoch 10/30\n",
      "473/473 [==============================] - 0s 721us/step - loss: 206670.5938 - val_loss: 222555.0156\n",
      "Epoch 11/30\n",
      "473/473 [==============================] - 0s 704us/step - loss: 163123.9062 - val_loss: 114619.1562\n",
      "Epoch 12/30\n",
      "473/473 [==============================] - 0s 738us/step - loss: 161919.4688 - val_loss: 845727.3125\n",
      "Epoch 13/30\n",
      "473/473 [==============================] - 0s 820us/step - loss: 174078.9062 - val_loss: 102007.2812\n",
      "Epoch 14/30\n",
      "473/473 [==============================] - 0s 713us/step - loss: 110031.3047 - val_loss: 256536.8438\n",
      "Epoch 15/30\n",
      "473/473 [==============================] - 0s 749us/step - loss: 127934.6172 - val_loss: 197390.0156\n",
      "Epoch 16/30\n",
      "473/473 [==============================] - 0s 708us/step - loss: 126532.1719 - val_loss: 224551.1875\n",
      "Epoch 17/30\n",
      "473/473 [==============================] - 0s 710us/step - loss: 134586.7812 - val_loss: 135100.0469\n",
      "263/263 [==============================] - 0s 519us/step - loss: 897416.4375\n",
      "[CV 3/3] END learning_rate=0.0029282007084415595, n_hidden=2, n_neurons=55, optimizer=sgd;, score=-897416.438 total time=   6.5s\n",
      "Epoch 1/30\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 66025576.0000 - val_loss: 8071532.5000\n",
      "Epoch 2/30\n",
      "473/473 [==============================] - 0s 827us/step - loss: 133558.3750 - val_loss: 92990.1953\n",
      "Epoch 3/30\n",
      "473/473 [==============================] - 0s 835us/step - loss: 89978.3438 - val_loss: 1633.0575\n",
      "Epoch 4/30\n",
      "473/473 [==============================] - 0s 896us/step - loss: 63528.2773 - val_loss: 395359.0938\n",
      "Epoch 5/30\n",
      "473/473 [==============================] - 0s 826us/step - loss: 65807.7109 - val_loss: 1272534.1250\n",
      "Epoch 6/30\n",
      "473/473 [==============================] - 0s 844us/step - loss: 55346.2031 - val_loss: 860626.4375\n",
      "Epoch 7/30\n",
      "473/473 [==============================] - 0s 1ms/step - loss: 54313.3438 - val_loss: 186273.6875\n",
      "Epoch 8/30\n",
      "473/473 [==============================] - 0s 848us/step - loss: 58685.6133 - val_loss: 77965.3594\n",
      "Epoch 9/30\n",
      "473/473 [==============================] - 0s 900us/step - loss: 49186.9727 - val_loss: 43254.9180\n",
      "Epoch 10/30\n",
      "473/473 [==============================] - 0s 844us/step - loss: 50537.0898 - val_loss: 2214634.5000\n",
      "Epoch 11/30\n",
      "473/473 [==============================] - 0s 889us/step - loss: 48224.6055 - val_loss: 1998419.6250\n",
      "Epoch 12/30\n",
      "473/473 [==============================] - 0s 1ms/step - loss: 43686.1758 - val_loss: 133341.2969\n",
      "Epoch 13/30\n",
      "473/473 [==============================] - 0s 920us/step - loss: 43621.1523 - val_loss: 504609.2500\n",
      "263/263 [==============================] - 0s 543us/step - loss: 100011.1172\n",
      "[CV 1/3] END learning_rate=0.01986178583868863, n_hidden=4, n_neurons=84, optimizer=sgd;, score=-100011.117 total time=   6.3s\n",
      "Epoch 1/30\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 83223672.0000 - val_loss: 636270.1250\n",
      "Epoch 2/30\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 170716.3125 - val_loss: 778955.5625\n",
      "Epoch 3/30\n",
      "473/473 [==============================] - 0s 972us/step - loss: 142119.5781 - val_loss: 840974.7500\n",
      "Epoch 4/30\n",
      "473/473 [==============================] - 0s 871us/step - loss: 134033.3750 - val_loss: 37079.0547\n",
      "Epoch 5/30\n",
      "473/473 [==============================] - 0s 848us/step - loss: 104320.6172 - val_loss: 96385.0156\n",
      "Epoch 6/30\n",
      "473/473 [==============================] - 0s 898us/step - loss: 88722.0234 - val_loss: 972733.0625\n",
      "Epoch 7/30\n",
      "473/473 [==============================] - 0s 839us/step - loss: 74834.4609 - val_loss: 78315.6641\n",
      "Epoch 8/30\n",
      "473/473 [==============================] - 0s 883us/step - loss: 81034.5234 - val_loss: 348757.6562\n",
      "Epoch 9/30\n",
      "473/473 [==============================] - 0s 851us/step - loss: 65266.4844 - val_loss: 143906.4844\n",
      "Epoch 10/30\n",
      "473/473 [==============================] - 0s 837us/step - loss: 65681.8906 - val_loss: 45520.4453\n",
      "Epoch 11/30\n",
      "473/473 [==============================] - 0s 923us/step - loss: 72260.5469 - val_loss: 279423.5312\n",
      "Epoch 12/30\n",
      "473/473 [==============================] - 0s 989us/step - loss: 62409.7734 - val_loss: 2562265.0000\n",
      "Epoch 13/30\n",
      "473/473 [==============================] - 0s 849us/step - loss: 67711.2031 - val_loss: 447613.1562\n",
      "Epoch 14/30\n",
      "473/473 [==============================] - 0s 915us/step - loss: 61872.5000 - val_loss: 88130.3828\n",
      "263/263 [==============================] - 0s 544us/step - loss: 75216.2344\n",
      "[CV 2/3] END learning_rate=0.01986178583868863, n_hidden=4, n_neurons=84, optimizer=sgd;, score=-75216.234 total time=   6.9s\n",
      "Epoch 1/30\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 3751595.5000 - val_loss: 118173.2188\n",
      "Epoch 2/30\n",
      "473/473 [==============================] - 0s 1ms/step - loss: 174219.4688 - val_loss: 129171.2031\n",
      "Epoch 3/30\n",
      "473/473 [==============================] - 0s 935us/step - loss: 101425.0078 - val_loss: 332925.0625\n",
      "Epoch 4/30\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 97787.6250 - val_loss: 782694.0625\n",
      "Epoch 5/30\n",
      "473/473 [==============================] - 0s 892us/step - loss: 85577.8750 - val_loss: 2445843.5000\n",
      "Epoch 6/30\n",
      "473/473 [==============================] - 0s 860us/step - loss: 86491.7266 - val_loss: 1209617.1250\n",
      "Epoch 7/30\n",
      "473/473 [==============================] - 1s 1ms/step - loss: 80200.0703 - val_loss: 163612.3594\n",
      "Epoch 8/30\n",
      "473/473 [==============================] - 0s 844us/step - loss: 69831.0938 - val_loss: 2068484.6250\n",
      "Epoch 9/30\n",
      "473/473 [==============================] - 0s 988us/step - loss: 69486.7422 - val_loss: 817689.8750\n",
      "Epoch 10/30\n",
      "473/473 [==============================] - 0s 875us/step - loss: 64615.6719 - val_loss: 1893449.2500\n",
      "Epoch 11/30\n",
      "473/473 [==============================] - 0s 924us/step - loss: 66941.6016 - val_loss: 745479.3750\n",
      "263/263 [==============================] - 0s 542us/step - loss: 110362.7891\n",
      "[CV 3/3] END learning_rate=0.01986178583868863, n_hidden=4, n_neurons=84, optimizer=sgd;, score=-110362.789 total time=   5.8s\n",
      "Epoch 1/30\n",
      "709/709 [==============================] - 1s 882us/step - loss: 256646960.0000 - val_loss: 1539230.3750\n",
      "Epoch 2/30\n",
      "709/709 [==============================] - 1s 805us/step - loss: 1077678.3750 - val_loss: 288719.8125\n",
      "Epoch 3/30\n",
      "709/709 [==============================] - 1s 772us/step - loss: 277002.1875 - val_loss: 369818.4375\n",
      "Epoch 4/30\n",
      "709/709 [==============================] - 1s 939us/step - loss: 137683.3281 - val_loss: 228841.1875\n",
      "Epoch 5/30\n",
      "709/709 [==============================] - 1s 774us/step - loss: 94984.6094 - val_loss: 335857.7812\n",
      "Epoch 6/30\n",
      "709/709 [==============================] - 1s 806us/step - loss: 156495.1094 - val_loss: 234089.2656\n",
      "Epoch 7/30\n",
      "709/709 [==============================] - 1s 866us/step - loss: 69400.0000 - val_loss: 278621.0312\n",
      "Epoch 8/30\n",
      "709/709 [==============================] - 1s 852us/step - loss: 87906.9453 - val_loss: 305151.8125\n",
      "Epoch 9/30\n",
      "709/709 [==============================] - 1s 769us/step - loss: 65616.0859 - val_loss: 195732.8438\n",
      "Epoch 10/30\n",
      "709/709 [==============================] - 1s 884us/step - loss: 59026.6758 - val_loss: 340859.1875\n",
      "Epoch 11/30\n",
      "709/709 [==============================] - 1s 1ms/step - loss: 55959.1133 - val_loss: 175594.7500\n",
      "Epoch 12/30\n",
      "709/709 [==============================] - 1s 801us/step - loss: 56152.9922 - val_loss: 193150.2500\n",
      "Epoch 13/30\n",
      "709/709 [==============================] - 1s 910us/step - loss: 50657.2148 - val_loss: 184911.4531\n",
      "Epoch 14/30\n",
      "709/709 [==============================] - 1s 862us/step - loss: 51612.0508 - val_loss: 541178.5625\n",
      "Epoch 15/30\n",
      "709/709 [==============================] - 1s 866us/step - loss: 50433.0859 - val_loss: 248874.4531\n",
      "Epoch 16/30\n",
      "709/709 [==============================] - 1s 817us/step - loss: 44272.7500 - val_loss: 136126.2344\n",
      "Epoch 17/30\n",
      "709/709 [==============================] - 1s 923us/step - loss: 29603.4121 - val_loss: 13748.0410\n",
      "Epoch 18/30\n",
      "709/709 [==============================] - 1s 873us/step - loss: 16908.4941 - val_loss: 86889.8828\n",
      "Epoch 19/30\n",
      "709/709 [==============================] - 1s 831us/step - loss: 13276.6943 - val_loss: 202593.8906\n",
      "Epoch 20/30\n",
      "709/709 [==============================] - 1s 864us/step - loss: 12952.6484 - val_loss: 3804.5625\n",
      "Epoch 21/30\n",
      "709/709 [==============================] - 1s 797us/step - loss: 15299.8701 - val_loss: 108501.2969\n",
      "Epoch 22/30\n",
      "709/709 [==============================] - 1s 888us/step - loss: 1739257.0000 - val_loss: 118944.0547\n",
      "Epoch 23/30\n",
      "709/709 [==============================] - 1s 835us/step - loss: 34233.6406 - val_loss: 290757.0000\n",
      "Epoch 24/30\n",
      "709/709 [==============================] - 1s 985us/step - loss: 20518.5391 - val_loss: 87699.9219\n",
      "Epoch 25/30\n",
      "709/709 [==============================] - 1s 826us/step - loss: 14671.3389 - val_loss: 62176.3945\n",
      "Epoch 26/30\n",
      "709/709 [==============================] - 1s 800us/step - loss: 11549.3525 - val_loss: 39521.7031\n",
      "Epoch 27/30\n",
      "709/709 [==============================] - 1s 817us/step - loss: 11713.6982 - val_loss: 33069.1094\n",
      "Epoch 28/30\n",
      "709/709 [==============================] - 1s 776us/step - loss: 11333.5293 - val_loss: 34716.1836\n",
      "Epoch 29/30\n",
      "709/709 [==============================] - 1s 800us/step - loss: 9601.0117 - val_loss: 93068.2969\n",
      "Epoch 30/30\n",
      "709/709 [==============================] - 1s 788us/step - loss: 10323.5537 - val_loss: 3435.0696\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-13 {color: black;}#sk-container-id-13 pre{padding: 0;}#sk-container-id-13 div.sk-toggleable {background-color: white;}#sk-container-id-13 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-13 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-13 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-13 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-13 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-13 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-13 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-13 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-13 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-13 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-13 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-13 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-13 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-13 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-13 div.sk-item {position: relative;z-index: 1;}#sk-container-id-13 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-13 div.sk-item::before, #sk-container-id-13 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-13 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-13 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-13 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-13 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-13 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-13 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-13 div.sk-label-container {text-align: center;}#sk-container-id-13 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-13 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-13\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x00000280DEE8F610&gt;,\n",
       "                   param_distributions={&#x27;learning_rate&#x27;: &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x0000028128819D00&gt;,\n",
       "                                        &#x27;n_hidden&#x27;: [1, 2, 3, 4],\n",
       "                                        &#x27;n_neurons&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
       "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
       "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
       "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
       "                                        &#x27;optimizer&#x27;: [&#x27;adam&#x27;, &#x27;sgd&#x27;]},\n",
       "                   verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-35\" type=\"checkbox\" ><label for=\"sk-estimator-id-35\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x00000280DEE8F610&gt;,\n",
       "                   param_distributions={&#x27;learning_rate&#x27;: &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x0000028128819D00&gt;,\n",
       "                                        &#x27;n_hidden&#x27;: [1, 2, 3, 4],\n",
       "                                        &#x27;n_neurons&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
       "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
       "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
       "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
       "                                        &#x27;optimizer&#x27;: [&#x27;adam&#x27;, &#x27;sgd&#x27;]},\n",
       "                   verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-36\" type=\"checkbox\" ><label for=\"sk-estimator-id-36\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x00000280DEE8F610&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-37\" type=\"checkbox\" ><label for=\"sk-estimator-id-37\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x00000280DEE8F610&gt;</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x00000280DEE8F610>,\n",
       "                   param_distributions={'learning_rate': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x0000028128819D00>,\n",
       "                                        'n_hidden': [1, 2, 3, 4],\n",
       "                                        'n_neurons': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
       "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
       "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
       "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
       "                                        'optimizer': ['adam', 'sgd']},\n",
       "                   verbose=3)"
      ]
     },
     "execution_count": 1853,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train, epochs=30,\n",
    " validation_split=0.1,\n",
    " callbacks=[keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1905,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          learning_rate  n_hidden  n_neurons optimizer\n",
      "0  0.021629088102836282         4         88      adam\n",
      "1 0.0007475736846458284         1         88       sgd\n",
      "2 0.0004076564878056142         3         59      adam\n",
      "3 0.0014732302002701152         2         92      adam\n",
      "4 0.0003650225911813686         3         53      adam\n",
      "5 0.0011018896957727365         3         70       sgd\n",
      "6 0.0009677735703570196         4          6      adam\n",
      "7 0.0005518963375383896         3         52       sgd\n",
      "8 0.0029282007084415595         2         55       sgd\n",
      "9   0.01986178583868863         4         84       sgd\n"
     ]
    }
   ],
   "source": [
    "results = grid.cv_results_\n",
    "# Convert the 'params' key from results to a DataFrame for better visualization\n",
    "params_df = pd.DataFrame(results['params'])\n",
    "\n",
    "# Print the DataFrame to see all hyperparameters used in each combination\n",
    "print(params_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1906,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: {'learning_rate': 0.0007475736846458284, 'n_hidden': 1, 'n_neurons': 88, 'optimizer': 'sgd'}\n",
      "Mean Test Score (Error): 1342225.02 +/- 1518257.40\n",
      "---\n",
      "Hyperparameters: {'learning_rate': 0.0029282007084415595, 'n_hidden': 2, 'n_neurons': 55, 'optimizer': 'sgd'}\n",
      "Mean Test Score (Error): 459893.76 +/- 339916.78\n",
      "---\n",
      "Hyperparameters: {'learning_rate': 0.0005518963375383896, 'n_hidden': 3, 'n_neurons': 52, 'optimizer': 'sgd'}\n",
      "Mean Test Score (Error): 360923.75 +/- 71582.65\n",
      "---\n",
      "Hyperparameters: {'learning_rate': 0.0011018896957727365, 'n_hidden': 3, 'n_neurons': 70, 'optimizer': 'sgd'}\n",
      "Mean Test Score (Error): 284306.71 +/- 168087.32\n",
      "---\n",
      "Hyperparameters: {'learning_rate': 0.0009677735703570196, 'n_hidden': 4, 'n_neurons': 6, 'optimizer': 'adam'}\n",
      "Mean Test Score (Error): 221143.13 +/- 245721.07\n",
      "---\n",
      "Hyperparameters: {'learning_rate': 0.0003650225911813686, 'n_hidden': 3, 'n_neurons': 53, 'optimizer': 'adam'}\n",
      "Mean Test Score (Error): 169506.79 +/- 85510.95\n",
      "---\n",
      "Hyperparameters: {'learning_rate': 0.01986178583868863, 'n_hidden': 4, 'n_neurons': 84, 'optimizer': 'sgd'}\n",
      "Mean Test Score (Error): 95196.71 +/- 14746.84\n",
      "---\n",
      "Hyperparameters: {'learning_rate': 0.0004076564878056142, 'n_hidden': 3, 'n_neurons': 59, 'optimizer': 'adam'}\n",
      "Mean Test Score (Error): 66674.79 +/- 60958.38\n",
      "---\n",
      "Hyperparameters: {'learning_rate': 0.021629088102836282, 'n_hidden': 4, 'n_neurons': 88, 'optimizer': 'adam'}\n",
      "Mean Test Score (Error): 49752.10 +/- 44121.60\n",
      "---\n",
      "Hyperparameters: {'learning_rate': 0.0014732302002701152, 'n_hidden': 2, 'n_neurons': 92, 'optimizer': 'adam'}\n",
      "Mean Test Score (Error): 24072.83 +/- 22279.43\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "# Sort the results by the mean_test_score (lower is better for loss functions)\n",
    "sorted_results_df = results_df.sort_values(by='mean_test_score', ascending=True)\n",
    "\n",
    "# Print the top 10 sets of hyperparameters and their corresponding error\n",
    "top_10_results = sorted_results_df.head(10)\n",
    "for index, row in top_10_results.iterrows():\n",
    "    params = row['params']\n",
    "    mean_test_score = row['mean_test_score']\n",
    "    std_test_score = row['std_test_score']\n",
    "    error = -mean_test_score  # Assuming lower is better for your scoring metric\n",
    "\n",
    "    print(f'Hyperparameters: {params}')\n",
    "    print(f'Mean Test Score (Error): {error:.2f} +/- {std_test_score:.2f}')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1907,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.wrappers.scikit_learn.KerasRegressor object at 0x00000280BC322D00>\n"
     ]
    }
   ],
   "source": [
    "best_model = grid.best_estimator_\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1908,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.0014732302002701152, 'n_hidden': 2, 'n_neurons': 92, 'optimizer': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "best_hyperparameters = grid.best_params_\n",
    "print(best_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1909,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-24072.825236002605\n"
     ]
    }
   ],
   "source": [
    "best_score = grid.best_score_\n",
    "print(best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1910,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "Date: 2023-08-24 19:50:00 | Prediction price: 488.29 | Real price: 406.00  | Error: 82.29\n",
      "Date: 2023-08-24 19:51:00 | Prediction price: 484.35 | Real price: 406.21  | Error: 78.14\n",
      "Date: 2023-08-24 19:52:00 | Prediction price: 482.14 | Real price: 406.11  | Error: 76.03\n",
      "Date: 2023-08-24 19:53:00 | Prediction price: 482.46 | Real price: 406.26  | Error: 76.20\n",
      "Date: 2023-08-24 19:54:00 | Prediction price: 490.97 | Real price: 406.30  | Error: 84.67\n",
      "Date: 2023-08-24 19:55:00 | Prediction price: 488.49 | Real price: 406.49  | Error: 82.00\n",
      "Date: 2023-08-24 19:56:00 | Prediction price: 497.69 | Real price: 406.55  | Error: 91.15\n",
      "Date: 2023-08-24 19:57:00 | Prediction price: 492.28 | Real price: 406.44  | Error: 85.84\n",
      "Date: 2023-08-24 19:58:00 | Prediction price: 497.66 | Real price: 406.43  | Error: 91.23\n",
      "Date: 2023-08-24 19:59:00 | Prediction price: 489.61 | Real price: 406.48  | Error: 83.13\n"
     ]
    }
   ],
   "source": [
    "# keras regressor doesnt have an .evaluate so we use the score\n",
    "# its opposite of the rmse so the higher the score the better\n",
    "\n",
    "y_pred = best_model.predict(X_test[-10:])\n",
    "\n",
    "# Get the corresponding real values for the same subset\n",
    "y_true = y_test[-10:]\n",
    "dates = df.index[-10:]\n",
    "# Convert the NumPy arrays to lists\n",
    "# Print values side by side\n",
    "for dt, pred, real in zip(dates, y_pred, y_true):\n",
    "    error = np.abs(pred - real[0])  # Calculate the absolute error\n",
    "    print(f'Date: {dt} | Prediction price: {pred:.2f} | Real price: {real[0]:.2f}  | Error: {error:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras regressor with sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 30  # we take every half an hour as a sequence\n",
    "num_features = X.shape[1]  # amount of features\n",
    "\n",
    "# Create sequences for training\n",
    "sequences = []\n",
    "close_arr = []\n",
    "\n",
    "for i in range(len(X) - sequence_length):\n",
    "    sequences.append(X.iloc[i:i+sequence_length].values)\n",
    "    close = y.iloc[i+sequence_length].values\n",
    "    close_arr.append(close)\n",
    "   \n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "sequences = np.array(sequences)\n",
    "close_arr = np.array(close_arr)\n",
    "\n",
    "# Reshape sequences for scaling\n",
    "sequences = sequences.reshape(-1, num_features)\n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler()\n",
    "sequences = scaler.fit_transform(sequences)  # Fit and transform sequences\n",
    "\n",
    "# Reshape sequences back to 3D\n",
    "sequences = sequences.reshape(-1, sequence_length, num_features)\n",
    "\n",
    "# Split data into training and test sets\n",
    "split_ratio = 0.9\n",
    "split_index = int(len(sequences) * split_ratio)\n",
    "\n",
    "X_train, X_test = sequences[:split_index], sequences[split_index:]\n",
    "y_train, y_test = close_arr[:split_index], close_arr[split_index:]\n",
    "\n",
    "\n",
    "\n",
    "param_grid = {\n",
    " 'n_hidden': [1, 2, 3, 4],\n",
    " #'n_hidden': np.arange(1, 20),\n",
    " 'n_neurons': np.arange(1, 100),\n",
    " 'learning_rate': reciprocal(3e-4, 3e-2),\n",
    "}\n",
    "\n",
    "model = KerasRegressor(build_model, batch_size=20)\n",
    "\n",
    "grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=10, cv=3, verbose=3)\n",
    "\n",
    "\n",
    "grid.fit(X_train, y_train, epochs=20,\n",
    " validation_split=0.1,\n",
    " callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grid.best_score_\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid.predict(X_test[200:201])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[200:201]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=(num_features_A,))\n",
    "input_B = keras.layers.Input(shape=(num_features_B,))\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, activation=\"linear\")(concat)\n",
    "\n",
    "aux_output = keras.layers.Dense(1)(hidden2)\n",
    "model = keras.models.Model(inputs=[input_A, input_B],\n",
    "outputs=[output, aux_output])\n",
    "X_train_A, X_train_B = X_train[:, :4], X_train[:, 4:]\n",
    "X_test_A, X_test_B = X_test[:, :4], X_test[:, 4:]\n",
    "X_new_A, X_new_B = X_test_A[:10], X_test_B[:10]\n",
    "model.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=\"sgd\")\n",
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
    "  validation_split=0.1, batch_size=32)\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns = df.shape[1]\n",
    "print(\"Number of columns:\", num_columns)\n",
    "#df['price_increased'] = np.where(df['close'].diff() > 0, 1, 0)\n",
    "#df['log_return'] = np.log(df['close'] / df['close'].shift(1))\n",
    "df['log_return'] = ta.log_return(df['close'])\n",
    "df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing X and y\n",
    "X = df.drop(columns=['close'])\n",
    "y = df[['close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the proportion of data to use for testing\n",
    "test_size = 0.2\n",
    "\n",
    "# Calculate the index where the split should occur\n",
    "split_index = int(len(X) * (1 - test_size))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l1\n",
    "\n",
    "# for regression\n",
    "input = keras.layers.Input(shape=(num_features,))\n",
    "hidden = keras.layers.Dense(30, activation=\"relu\", kernel_regularizer=l1(0.01))(input)\n",
    "concat = keras.layers.Concatenate()([input, hidden])\n",
    "output = keras.layers.Dense(1, activation=\"linear\", kernel_regularizer=l1(0.01))(concat)\n",
    "model = keras.models.Model(inputs=[input], outputs=[output])\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "           validation_split=0.1, batch_size=32 )\n",
    "\n",
    "\"\"\" # for classification\n",
    "input = keras.layers.Input(shape=(num_features,))\n",
    "hidden = keras.layers.Dense(30, activation=\"relu\")(input)\n",
    "concat = keras.layers.Concatenate()([input, hidden])\n",
    "output = keras.layers.Dense(1, activation=\"sigmoid\")(concat)\n",
    "model = keras.models.Model(inputs=[input], outputs=[output])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=\"accuracy\")\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "           validation_split=0.1, batch_size=32 ) \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making predictions\n",
    "X_new = X_test[270:280]\n",
    "predictions = model.predict(X_new)\n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_test[270:280])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[270:280]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have X_train and column_names defined\n",
    "\n",
    "# Get the weights of the input layer\n",
    "column_names = X.columns.tolist()\n",
    "input_weights = model.layers[1].get_weights()[0]  # Only the weight matrix\n",
    "\n",
    "# Create a dictionary to map weights to column names\n",
    "weights_by_column = dict(zip(column_names, input_weights))\n",
    "\n",
    "# Print the weights associated with each feature\n",
    "for feature, weight in weights_by_column.items():\n",
    "    print(f\"Feature: {feature}, Weight: {weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming you have X_train and column_names defined\n",
    "# Assuming you have model defined\n",
    "\n",
    "# Get the weights of the input layer\n",
    "input_weights = model.layers[1].get_weights()[0]  # Only the weight matrix\n",
    "\n",
    "# Create a dictionary to map weights to column names\n",
    "weights_by_column = dict(zip(column_names, input_weights.T))  # Transpose weights\n",
    "\n",
    "# Calculate the absolute sum of weights for each feature\n",
    "feature_weights = {feature: np.abs(weight).sum() for feature, weight in weights_by_column.items()}\n",
    "\n",
    "# Number of top features to select\n",
    "top_n = 30\n",
    "\n",
    "# Get the top 'n' features based on weights\n",
    "top_features = sorted(feature_weights, key=lambda x: feature_weights[x], reverse=True)[:top_n]\n",
    "\n",
    "# Print the top features and their weights\n",
    "for feature in top_features:\n",
    "    print(f\"Feature: {feature}, Weight: {feature_weights[feature]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1776,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    options = {\"input_shape\": input_shape}\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\", **options))\n",
    "        options = {}\n",
    "    model.add(keras.layers.Dense(1, **options))\n",
    "    optimizer = keras.optimizers.SGD(learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=(8,)):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))  # Add input layer\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1777,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ali\\AppData\\Local\\Temp\\ipykernel_13696\\2492287859.py:1: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model(input_shape=(num_features,)))\n"
     ]
    }
   ],
   "source": [
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model(input_shape=(num_features,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_reg.fit(X_train, y_train, epochs=100,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "y_pred = keras_reg.predict(X_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=(8,)):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))  # Add input layer\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n",
    "\n",
    "keras_reg.fit(X_train, y_train, epochs=100,\n",
    "              validation_split=0.1,\n",
    "              callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "y_pred = keras_reg.predict(X_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
